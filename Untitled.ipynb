{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd51126-661b-4d78-a05b-72ff992fe52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56804725-699f-415c-be3d-112a3b5c6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Layer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient layer for S4Ms. (Structured State Space Sequence Models).\n",
    "    Implements initialization of A as a NPLR matrix, enabling fast \n",
    "    matrix vector multiplication. \n",
    "\n",
    "    Several parameters, such as the projection matrix, are learned.\n",
    "\n",
    "    In this case, the C matrix is actually learned as C(1-A^L). \n",
    "    This is fairly easy to undo, and is done in the calc of Cbar.\n",
    "\n",
    "    Parameters:\n",
    "        N_input : dimension of input,\n",
    "        latent_dim : int, dimensions of latent space,\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        N_input : int,\n",
    "        latent_dim : int,\n",
    "        dt_min  : torch.Tensor = torch.tensor(0.001),\n",
    "        dt_max  : torch.Tensor = torch.tensor(0.1),\n",
    "        step_grad : bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert N_input==1\n",
    "\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.Lambda, self.P, self.B, _ = self.make_DPLR_HiPPO(self.latent_dim)\n",
    "        \n",
    "        self.Lambda = torch.autograd.Variable(self.Lambda, requires_grad = True)\n",
    "        self.P = torch.autograd.Variable(self.P, requires_grad = True)\n",
    "        self.B = torch.autograd.Variable(self.B, requires_grad = True)\n",
    "        \n",
    "        self.log_dt = self.log_step_initializer(dt_min, dt_max, step_grad)\n",
    "        \n",
    "        Ctilde = torch.nn.init.normal_(torch.empty(self.latent_dim, 2), mean=0, std=0.5**0.5)\n",
    "        self.Ctilde = torch.autograd.Variable(Ctilde[:,0] + Ctilde[:,1]*1j, requires_grad=True)\n",
    "\n",
    "        self.D = torch.autograd.Variable(torch.tensor(1.), requires_grad = True)\n",
    "\n",
    "    def make_HiPPO(self, N : int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        creates HiPPO matrix for legendre polynomials up to order N\n",
    "        parameters:\n",
    "            N: int\n",
    "        \"\"\"\n",
    "        P = torch.sqrt(1+2*torch.arange(N))\n",
    "        A = P.unsqueeze(1) * P.unsqueeze(0)\n",
    "        A = torch.tril(A) - torch.diag(torch.arange(N))\n",
    "        return -A\n",
    "        \n",
    "    def make_NPLR_HiPPO(self, N : int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        creating hippo matrix and associated low rank additive component, P\n",
    "        and the B matrix associated, as hippo forces it\n",
    "    \n",
    "        parameters:\n",
    "            N : int, degree of legendre polynomial coefficient\n",
    "        \"\"\"\n",
    "        nhippo = self.make_HiPPO(N)\n",
    "    \n",
    "        P = torch.sqrt(torch.arange(N)+0.5).to(torch.complex64)\n",
    "        B = torch.sqrt(2*torch.arange(N)+1.0).to(torch.complex64)\n",
    "    \n",
    "        return nhippo.to(torch.complex64), P, B\n",
    "\n",
    "    def make_DPLR_HiPPO(self, N : int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        convert matrices to DPLR representation\n",
    "        parameters:\n",
    "            N : int, degree of legendre polynomials\n",
    "        \"\"\"\n",
    "        A, P, B = self.make_NPLR_HiPPO(N)\n",
    "    \n",
    "        S = A + torch.outer(P, P)\n",
    "    \n",
    "        S_diag = torch.diagonal(S)\n",
    "        Lambda_real = torch.mean(S_diag) * torch.ones_like(S_diag)\n",
    "    \n",
    "        Lambda_imag, V = torch.linalg.eigh(S * -1j)\n",
    "        P = V.T.conj() @ P\n",
    "        B = V.T.conj() @ B\n",
    "        return Lambda_real + 1j * Lambda_imag, P, B, V\n",
    "\n",
    "    \n",
    "    def log_step_initializer(self, dt_min = torch.tensor(0.001), dt_max = torch.tensor(0.1), requires_grad = True):\n",
    "        \"\"\"\n",
    "        initial guess for dt, from random number generator. to be learned.\n",
    "    \n",
    "        parameters:\n",
    "            dt_min\n",
    "            dt_max\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.rand(1) * (torch.log(dt_max) - torch.log(dt_min)) + torch.log(dt_min), requires_grad = requires_grad)\n",
    "\n",
    "    \n",
    "    def K_gen_DPLR(\n",
    "        self,\n",
    "        Lambda : torch.Tensor, \n",
    "        P : torch.Tensor, \n",
    "        Q : torch.Tensor, \n",
    "        B: torch.Tensor, \n",
    "        C : torch.Tensor, \n",
    "        delta : torch.Tensor, \n",
    "        L : int\n",
    "    )-> torch.Tensor:\n",
    "        \"\"\"\n",
    "        computes convolution kernel from generating function using DPLR representation and\n",
    "        the cauchy kernel\n",
    "    \n",
    "        Parameters:\n",
    "            Lambda : diagonal part of DPLR\n",
    "            P : N matrix, rank 1 representation to A\n",
    "            Q : N matrix, rank 1 representation to A\n",
    "            C : N matrix, projection from latent to input\n",
    "            B : N matrix, projection from input to latent\n",
    "        \"\"\"\n",
    "        Omega_L = torch.exp(-2j*torch.pi * (torch.arange(L))/L)\n",
    "    \n",
    "        aterm = (torch.conj(C), torch.conj(Q))\n",
    "        bterm = (B, P)\n",
    "    \n",
    "        g = (2.0/delta) * ((1.0-Omega_L)/(1.0+Omega_L))\n",
    "        c = 2.0 / (1.0+Omega_L)\n",
    "    \n",
    "        k00 = self.cauchy((aterm[0] * bterm[0]).unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "        k01 = self.cauchy((aterm[0] * bterm[1]).unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "        k10 = self.cauchy((aterm[1] * bterm[0]).unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "        k11 = self.cauchy((aterm[1] * bterm[1]).unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "    \n",
    "        atRoots = c * (k00 - k01 * (1.0 / (1.0 + k11)) * k10)\n",
    "        out = torch.fft.ifft(atRoots, L)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def cauchy(self, k : torch.Tensor, omega : torch.Tensor, lambd : torch.Tensor):\n",
    "        \"\"\"\n",
    "        computes cauchy kernel \n",
    "        sum(c_i * b_i/(z - lambda_i)\n",
    "\n",
    "        Parameters:\n",
    "            k : term by term dot product of vectors\n",
    "            omega : function of the roots of unity\n",
    "            lambd: diagonal parts of the DPLR matrix\n",
    "        \"\"\"\n",
    "        return torch.sum(k/(omega-lambd), axis=1)\n",
    "\n",
    "    def discrete_DPLR(\n",
    "        self,\n",
    "        Lambda : torch.Tensor,\n",
    "        P : torch.Tensor,\n",
    "        Q : torch.Tensor,\n",
    "        B : torch.Tensor,\n",
    "        C : torch.Tensor,\n",
    "        delta : torch.Tensor,\n",
    "        L : int\n",
    "    )->(torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        computes the discretized version of the state space model,\n",
    "        assuming the DPLR form\n",
    "    \n",
    "        Parameters:\n",
    "            Lambda : Nx1, represents the diagonal values of the A matrix\n",
    "            P : Nx1, represents part of the low rank aspect of the A matrix\n",
    "            Q : Nx1, represents the other part of the low rank aspect of the A matrix\n",
    "            B : N, projection from input to latent\n",
    "            C : N, projection from latent to input\n",
    "            delta : step size\n",
    "            L : length of window\n",
    "        \"\"\"\n",
    "        Bt = B.unsqueeze(1)\n",
    "        Ct = C.unsqueeze(0)\n",
    "    \n",
    "        A = (torch.diag(Lambda) - torch.outer(P, torch.conj(Q)))\n",
    "        A0 = 2.0/delta * torch.eye(A.shape[0]) + A\n",
    "        \n",
    "        Qdagger = Q.conj().unsqueeze(0)\n",
    "        P_1 = P.unsqueeze(1)\n",
    "        \n",
    "        D = torch.diag(1.0/(2.0/delta - Lambda))\n",
    "        A1 = (D -  (1.0/(1.0 + Qdagger @ D @ P_1)) * D@P_1@Qdagger@D)\n",
    "        Ab = A1@A0\n",
    "        Bb = 2 * A1@B.unsqueeze(1)\n",
    "        Cb = Ct @ torch.conj(torch.linalg.inv(torch.eye(A.shape[0]) - torch.matrix_power(Ab, L)))\n",
    "        return Ab, Bb, Cb.conj()\n",
    "\n",
    "    def scan_SSM(\n",
    "        self,\n",
    "        Ab : torch.Tensor,\n",
    "        Bb : torch.Tensor,\n",
    "        Cb : torch.Tensor,\n",
    "        u  : torch.Tensor,\n",
    "        x0 : torch.Tensor,\n",
    "    ):\n",
    "        x = torch.zeros((u.shape[0], Ab.shape[0]))\n",
    "        y = torch.zeros_like(u)\n",
    "\n",
    "        for index in range(u.shape[0]):\n",
    "            x[index, :] = \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        u : torch.Tensor,\n",
    "        mode : str,\n",
    "        x0 : torch.Tensor = torch.Tensor(0),\n",
    "    ):\n",
    "        dt = torch.exp(self.log_dt)\n",
    "        L = u.shape[0]\n",
    "        if mode == \"recurrent\":\n",
    "            Ab, Bb, Cb = self.discrete_DPLR(self.Lambda, self.P, self.P, self.B, self.C, dt, L)\n",
    "            return \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6847a6e-536d-4dab-818d-3da9b6383684",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = S4Layer(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97cab604-f861-41d1-ada7-335f618d4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy(k : torch.Tensor, omega : torch.Tensor, lambd : torch.Tensor):\n",
    "    \"\"\"\n",
    "    computes cauchy kernel \n",
    "    sum(c_i * b_i/(z - lambda_i)\n",
    "\n",
    "    Parameters:\n",
    "        k : term by term dot product of vectors\n",
    "        omega : function of the roots of unity\n",
    "        lambd: diagonal parts of the DPLR matrix\n",
    "    \"\"\"\n",
    "    return torch.sum(k/(omega-lambd), axis=1)\n",
    "\n",
    "def K_gen_DPLR(\n",
    "    Lambda : torch.Tensor, \n",
    "    P : torch.Tensor, \n",
    "    Q : torch.Tensor, \n",
    "    B: torch.Tensor, \n",
    "    C : torch.Tensor, \n",
    "    delta : torch.Tensor, \n",
    "    L : int\n",
    ")-> torch.Tensor:\n",
    "    \"\"\"\n",
    "    computes convolution kernel from generating function using DPLR representation and\n",
    "    the cauchy kernel\n",
    "\n",
    "    Parameters:\n",
    "        Lambda : diagonal part of DPLR\n",
    "        P : N matrix, rank 1 representation to A\n",
    "        Q : N matrix, rank 1 representation to A\n",
    "        C : N matrix, projection from latent to input\n",
    "        B : N matrix, projection from input to latent\n",
    "    \"\"\"\n",
    "    Omega_L = torch.exp(-2j*torch.pi * (torch.arange(L))/L)\n",
    "\n",
    "    aterm = (torch.conj(C), torch.conj(Q))\n",
    "    bterm = (B, P)\n",
    "\n",
    "    g = (2.0/delta) * ((1.0-Omega_L)/(1.0+Omega_L))\n",
    "    c = 2.0 / (1.0+Omega_L)\n",
    "\n",
    "    k00 = cauchy((aterm[0] * bterm[0]).unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "    k01 = cauchy((aterm[0] * bterm[1]).unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "    k10 = cauchy((aterm[1] * bterm[0]).unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "    k11 = cauchy((aterm[1] * bterm[1]).unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "\n",
    "    atRoots = c * (k00 - k01 * (1.0 / (1.0 + k11)) * k10)\n",
    "    out = torch.fft.ifft(atRoots, L)[:L]\n",
    "    return out.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae0c7dad-39b8-4094-85a2-8ff746632080",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = torch.randn(10)\n",
    "P = torch.randn(10)\n",
    "\n",
    "A = torch.diag(Lambda) - torch.outer(P, P)\n",
    "\n",
    "B = torch.randn(10)\n",
    "C = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1c133dc-e492-4f68-b677-0786aa6ebf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = K_gen_DPLR(Lambda, P, P, B, C, torch.tensor(.01), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e95999a-0b45-471c-b85f-8943ad0eeb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "Ab, Bb, Cb = discrete_DPLR(Lambda, P, P, B, C, torch.tensor(.01), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "799870e2-0c77-46f8-82bb-86019663dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_conv(Ab, Bb, Cb, L):\n",
    "        return torch.stack([(Cb @ torch.matrix_power(Ab, l) @ Bb).squeeze() for l in range(L)]\n",
    "    )\n",
    "\n",
    "test_2 = K_conv(Ab, Bb, Cb, L=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ddeffdcd-c9e1-4add-80b7-1d876bdebed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1590, -0.1603, -0.1614, -0.1625, -0.1636, -0.1646, -0.1655, -0.1664,\n",
       "        -0.1672, -0.1680, -0.1687, -0.1694, -0.1701, -0.1707, -0.1712, -0.1718,\n",
       "        -0.1722, -0.1727, -0.1731, -0.1735, -0.1739, -0.1742, -0.1745, -0.1748,\n",
       "        -0.1750, -0.1753, -0.1755, -0.1756, -0.1758, -0.1759, -0.1761, -0.1761,\n",
       "        -0.1762, -0.1763, -0.1763, -0.1764, -0.1764, -0.1764, -0.1763, -0.1763,\n",
       "        -0.1763, -0.1762, -0.1761, -0.1760, -0.1759, -0.1758, -0.1757, -0.1756,\n",
       "        -0.1754, -0.1753, -0.1751, -0.1750, -0.1748, -0.1746, -0.1744, -0.1742,\n",
       "        -0.1740, -0.1738, -0.1736, -0.1733, -0.1731, -0.1729, -0.1726, -0.1724,\n",
       "        -0.1721, -0.1718, -0.1716, -0.1713, -0.1710, -0.1707, -0.1704, -0.1701,\n",
       "        -0.1698, -0.1695, -0.1692, -0.1689, -0.1686, -0.1683, -0.1679, -0.1676,\n",
       "        -0.1673, -0.1669, -0.1666, -0.1662, -0.1659, -0.1655, -0.1652, -0.1648,\n",
       "        -0.1645, -0.1641, -0.1637, -0.1634, -0.1630, -0.1626, -0.1622, -0.1618,\n",
       "        -0.1614, -0.1610, -0.1606, -0.1602])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1ddac5a-b70b-442c-b876-da58428000b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1591, -0.1603, -0.1615, -0.1626, -0.1637, -0.1646, -0.1656, -0.1665,\n",
       "        -0.1673, -0.1681, -0.1688, -0.1695, -0.1701, -0.1707, -0.1713, -0.1718,\n",
       "        -0.1723, -0.1728, -0.1732, -0.1736, -0.1740, -0.1743, -0.1746, -0.1749,\n",
       "        -0.1751, -0.1753, -0.1755, -0.1757, -0.1759, -0.1760, -0.1761, -0.1762,\n",
       "        -0.1763, -0.1764, -0.1764, -0.1764, -0.1764, -0.1764, -0.1764, -0.1764,\n",
       "        -0.1763, -0.1763, -0.1762, -0.1761, -0.1760, -0.1759, -0.1758, -0.1757,\n",
       "        -0.1755, -0.1754, -0.1752, -0.1751, -0.1749, -0.1747, -0.1745, -0.1743,\n",
       "        -0.1741, -0.1739, -0.1737, -0.1734, -0.1732, -0.1729, -0.1727, -0.1724,\n",
       "        -0.1722, -0.1719, -0.1716, -0.1714, -0.1711, -0.1708, -0.1705, -0.1702,\n",
       "        -0.1699, -0.1696, -0.1693, -0.1690, -0.1687, -0.1683, -0.1680, -0.1677,\n",
       "        -0.1674, -0.1670, -0.1667, -0.1663, -0.1660, -0.1656, -0.1653, -0.1649,\n",
       "        -0.1645, -0.1642, -0.1638, -0.1634, -0.1631, -0.1627, -0.1623, -0.1619,\n",
       "        -0.1615, -0.1611, -0.1607, -0.1603])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0253f520-a628-43c3-964d-40db1aad297e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7949+1.1499j,  0.3463+1.0878j,  1.5644+0.3168j,  1.9604-0.4458j,\n",
       "          0.4461-0.8018j,  0.5388-0.9901j, -1.0271-0.5473j, -0.9828+0.1719j,\n",
       "          1.5598-3.8654j,  0.9211+0.7982j]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.conj(test.Ctilde) * test.B.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7859c35-7f41-4584-bb38-b513ee7596f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7949+1.1499j,  0.3463+1.0878j,  1.5644+0.3168j,  1.9604-0.4458j,\n",
       "          0.4461-0.8018j,  0.5388-0.9901j, -1.0271-0.5473j, -0.9828+0.1719j,\n",
       "          1.5598-3.8654j,  0.9211+0.7982j]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.conj(test.Ctilde) * test.B).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "183aaebc-b1dd-4025-b126-b5cb885c29f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0705, -0.0054,  0.0152,  0.0349, -0.0133,  0.0218,  0.0081, -0.0070,\n",
       "         0.0184, -0.0053,  0.0042,  0.0091, -0.0041,  0.0104,  0.0024,  0.0047,\n",
       "         0.0097,  0.0039,  0.0119,  0.0075,  0.0114,  0.0132,  0.0100,  0.0181,\n",
       "         0.0116,  0.0173,  0.0189,  0.0131,  0.0240,  0.0157,  0.0196,  0.0249,\n",
       "         0.0143,  0.0269,  0.0199,  0.0185,  0.0289,  0.0149,  0.0258,  0.0236,\n",
       "         0.0153,  0.0297,  0.0155,  0.0214,  0.0258,  0.0117,  0.0269,  0.0163,\n",
       "         0.0149,  0.0256,  0.0086,  0.0211,  0.0169,  0.0081,  0.0229,  0.0066,\n",
       "         0.0135,  0.0166,  0.0021,  0.0179,  0.0056,  0.0053,  0.0150, -0.0023,\n",
       "         0.0112,  0.0052, -0.0021,  0.0120, -0.0048,  0.0037,  0.0050, -0.0080,\n",
       "         0.0077, -0.0056, -0.0037,  0.0043, -0.0118,  0.0024, -0.0051, -0.0099,\n",
       "         0.0029, -0.0134, -0.0032, -0.0039, -0.0145,  0.0007, -0.0131, -0.0086,\n",
       "        -0.0027, -0.0170, -0.0022, -0.0112, -0.0130, -0.0019, -0.0174, -0.0055,\n",
       "        -0.0084, -0.0161, -0.0018, -0.0159], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_gen_DPLR(test.Lambda, test.P, test.P, test.B, test.Ctilde, test.dt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f40b9bf7-be66-484d-9702-09d2006d92da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0379-0.0367j]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cb@(torch.matrix_power(Ab, 2))@Bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f1e00eb-5669-42a9-8c66-fba66f87cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bloop = test.K_gen_DPLR(test.Lambda, test.P, test.P, test.B, test.Ctilde, test.dt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "551c385c-4cca-48ba-8fe8-9c9501a130a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0066-1.7582e-02j, -0.0056-1.8130e-02j, -0.0047-1.8441e-02j,\n",
       "        -0.0039-1.8544e-02j, -0.0032-1.8465e-02j, -0.0027-1.8229e-02j,\n",
       "        -0.0022-1.7859e-02j, -0.0018-1.7374e-02j, -0.0015-1.6793e-02j,\n",
       "        -0.0013-1.6132e-02j, -0.0011-1.5407e-02j, -0.0010-1.4632e-02j,\n",
       "        -0.0009-1.3819e-02j, -0.0009-1.2978e-02j, -0.0009-1.2121e-02j,\n",
       "        -0.0010-1.1255e-02j, -0.0010-1.0388e-02j, -0.0011-9.5279e-03j,\n",
       "        -0.0012-8.6801e-03j, -0.0013-7.8499e-03j, -0.0015-7.0419e-03j,\n",
       "        -0.0016-6.2601e-03j, -0.0018-5.5077e-03j, -0.0020-4.7875e-03j,\n",
       "        -0.0021-4.1017e-03j, -0.0023-3.4522e-03j, -0.0025-2.8403e-03j,\n",
       "        -0.0027-2.2670e-03j, -0.0028-1.7330e-03j, -0.0030-1.2388e-03j,\n",
       "        -0.0032-7.8424e-04j, -0.0034-3.6936e-04j, -0.0035+6.2415e-06j,\n",
       "        -0.0037+3.4313e-04j, -0.0038+6.4201e-04j, -0.0040+9.0376e-04j,\n",
       "        -0.0042+1.1294e-03j, -0.0043+1.3199e-03j, -0.0044+1.4767e-03j,\n",
       "        -0.0046+1.6008e-03j, -0.0047+1.6938e-03j, -0.0048+1.7568e-03j,\n",
       "        -0.0050+1.7915e-03j, -0.0051+1.7991e-03j, -0.0052+1.7813e-03j,\n",
       "        -0.0053+1.7395e-03j, -0.0054+1.6751e-03j, -0.0055+1.5896e-03j,\n",
       "        -0.0056+1.4846e-03j, -0.0057+1.3614e-03j, -0.0058+1.2215e-03j,\n",
       "        -0.0059+1.0663e-03j, -0.0060+8.9708e-04j, -0.0060+7.1530e-04j,\n",
       "        -0.0061+5.2222e-04j, -0.0062+3.1909e-04j, -0.0063+1.0713e-04j,\n",
       "        -0.0063-1.1247e-04j, -0.0064-3.3859e-04j, -0.0065-5.7013e-04j,\n",
       "        -0.0065-8.0606e-04j, -0.0066-1.0454e-03j, -0.0067-1.2871e-03j,\n",
       "        -0.0067-1.5304e-03j, -0.0068-1.7744e-03j, -0.0068-2.0182e-03j,\n",
       "        -0.0069-2.2612e-03j, -0.0069-2.5026e-03j, -0.0070-2.7417e-03j,\n",
       "        -0.0070-2.9779e-03j, -0.0071-3.2106e-03j, -0.0071-3.4393e-03j,\n",
       "        -0.0072-3.6634e-03j, -0.0072-3.8826e-03j, -0.0073-4.0964e-03j,\n",
       "        -0.0073-4.3044e-03j, -0.0074-4.5062e-03j, -0.0074-4.7017e-03j,\n",
       "        -0.0075-4.8904e-03j, -0.0075-5.0721e-03j, -0.0075-5.2467e-03j,\n",
       "        -0.0076-5.4140e-03j, -0.0076-5.5737e-03j, -0.0077-5.7258e-03j,\n",
       "        -0.0077-5.8702e-03j, -0.0078-6.0068e-03j, -0.0078-6.1355e-03j,\n",
       "        -0.0078-6.2563e-03j, -0.0079-6.3693e-03j, -0.0079-6.4744e-03j,\n",
       "        -0.0080-6.5716e-03j, -0.0080-6.6610e-03j, -0.0080-6.7428e-03j,\n",
       "        -0.0081-6.8169e-03j, -0.0081-6.8834e-03j, -0.0081-6.9425e-03j,\n",
       "        -0.0082-6.9943e-03j, -0.0082-7.0389e-03j, -0.0082-7.0764e-03j,\n",
       "        -0.0083-7.1071e-03j], grad_fn=<FftC2CBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f63e547e-6807-4e7d-b5bb-7e48abf3ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ab, Bb, Cb = test.discrete_DPLR(test.Lambda, test.P, test.P, test.B, test.Ctilde, test.dt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699c111b-7be3-4276-9c81-e87819998070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0312, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloop[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bba04-b8c1-428b-871a-578a52b156d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "545d5da8-8339-4adf-88bc-bf51d894222e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f892f64-394f-477f-a3be-d0ba71ad8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_DPLR(Lambda, P, Q, B, C, step, L):\n",
    "    # Convert parameters to matrices\n",
    "    B = B.unsqueeze(1)\n",
    "    Ct = C.unsqueeze(0)\n",
    "\n",
    "    N = Lambda.shape[0]\n",
    "    A = torch.diag(Lambda) - torch.outer(P, Q.conj())\n",
    "    I = torch.eye(N)\n",
    "\n",
    "    # Forward Euler\n",
    "    A0 = (2.0 / step) * I + A\n",
    "    print(A0.shape)\n",
    "    # Backward Euler\n",
    "    D = torch.diag(1.0 / ((2.0 / step) - Lambda))\n",
    "    print(D.shape)\n",
    "    Qc = torch.conj(Q.unsqueeze(0))\n",
    "    print(Qc.shape)\n",
    "    P2 = P.reshape(-1, 1)\n",
    "    A1 = D - (D @ P2 * (1.0 / (1 + (Qc @ D @ P2))) * Qc @ D)\n",
    "    print(A1.shape)\n",
    "    # A bar and B bar\n",
    "    Ab = A1 @ A0\n",
    "    Bb = 2 * A1 @ B\n",
    "\n",
    "    # Recover Cbar from Ct\n",
    "    Cb = Ct @ torch.linalg.inv(I - torch.matrix_power(Ab, L)).conj()\n",
    "    return Ab, Bb, Cb.conj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8bebd0c-22d7-46fa-85b4-77d9520813e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "Ab, Bb, Cb = discrete_DPLR(test.Lambda, test.P, test.P, test.B, test.Ctilde, test.dt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e878cfbb-b4ec-4446-9d75-9748d42d5835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.7959e-01-0.3620j, -7.7162e-02+0.0219j,  5.0469e-02-0.0087j,\n",
       "         -3.7691e-02-0.0004j,  2.1994e-02+0.0162j,  1.1074e-02-0.0250j,\n",
       "          3.2512e-02-0.0191j,  4.8135e-02-0.0175j,  7.7887e-02-0.0192j,\n",
       "          2.2241e-01-0.0071j],\n",
       "        [-7.5232e-02+0.0279j,  9.5305e-01-0.1394j,  1.8449e-02-0.0007j,\n",
       "         -1.3449e-02-0.0020j,  7.0893e-03+0.0068j,  5.1457e-03-0.0084j,\n",
       "          1.2527e-02-0.0053j,  1.8034e-02-0.0040j,  2.8749e-02-0.0032j,\n",
       "          7.9818e-02+0.0081j],\n",
       "        [ 4.7530e-02-0.0191j,  1.8192e-02-0.0032j,  9.7820e-01-0.0639j,\n",
       "          8.6155e-03+0.0010j, -4.6412e-03-0.0042j, -3.1396e-03+0.0054j,\n",
       "         -7.9033e-03+0.0036j, -1.1440e-02+0.0028j, -1.8291e-02+0.0025j,\n",
       "         -5.1073e-02-0.0038j],\n",
       "        [-3.2603e-02+0.0189j, -1.2917e-02+0.0042j,  8.4858e-03-0.0018j,\n",
       "          9.8532e-01-0.0268j,  3.8287e-03+0.0026j,  1.7110e-03-0.0043j,\n",
       "          5.3783e-03-0.0034j,  8.0331e-03-0.0033j,  1.3058e-02-0.0038j,\n",
       "          3.7600e-02-0.0027j],\n",
       "        [ 1.1709e-02-0.0247j,  5.9818e-03-0.0078j, -4.3275e-03+0.0046j,\n",
       "          3.7398e-03-0.0027j,  9.8870e-01-0.0063j,  7.4017e-04+0.0033j,\n",
       "         -1.8000e-03+0.0043j, -3.4534e-03+0.0052j, -6.2533e-03+0.0076j,\n",
       "         -2.1359e-02+0.0170j],\n",
       "        [ 2.1358e-02+0.0170j,  6.2532e-03+0.0076j, -3.4533e-03-0.0052j,\n",
       "          1.8000e-03+0.0043j,  7.4017e-04-0.0033j,  9.8870e-01+0.0063j,\n",
       "         -3.7398e-03-0.0027j, -4.3275e-03-0.0046j, -5.9818e-03-0.0078j,\n",
       "         -1.1709e-02-0.0247j],\n",
       "        [ 3.7600e-02+0.0027j,  1.3058e-02+0.0038j, -8.0331e-03-0.0033j,\n",
       "          5.3783e-03+0.0034j, -1.7111e-03-0.0043j, -3.8287e-03+0.0026j,\n",
       "          9.8532e-01+0.0268j, -8.4858e-03-0.0018j, -1.2917e-02-0.0042j,\n",
       "         -3.2603e-02-0.0189j],\n",
       "        [ 5.1073e-02-0.0038j,  1.8291e-02+0.0025j, -1.1440e-02-0.0028j,\n",
       "          7.9033e-03+0.0036j, -3.1397e-03-0.0054j, -4.6412e-03+0.0042j,\n",
       "         -8.6155e-03+0.0010j,  9.7820e-01+0.0639j, -1.8192e-02-0.0032j,\n",
       "         -4.7530e-02-0.0191j],\n",
       "        [ 7.9818e-02-0.0081j,  2.8749e-02+0.0032j, -1.8034e-02-0.0040j,\n",
       "          1.2527e-02+0.0053j, -5.1457e-03-0.0084j, -7.0893e-03+0.0068j,\n",
       "         -1.3449e-02+0.0020j, -1.8449e-02-0.0007j,  9.5305e-01+0.1394j,\n",
       "         -7.5232e-02-0.0279j],\n",
       "        [ 2.2241e-01+0.0071j,  7.7887e-02+0.0192j, -4.8135e-02-0.0175j,\n",
       "          3.2512e-02+0.0191j, -1.1074e-02-0.0250j, -2.1994e-02+0.0162j,\n",
       "         -3.7691e-02+0.0004j, -5.0469e-02-0.0087j, -7.7162e-02-0.0219j,\n",
       "          6.7959e-01+0.3620j]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c20c22fe-1dc8-44a7-b281-cb3c7a3d898b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0056-0.0181j, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloop[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1b3af93-8a88-4e8a-8e45-becf529d059e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0056-0.0181j]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cb@torch.matrix_power(Ab, 1)@Bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6e2156f7-9435-4288-ab24-42f4c34d417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_conv(Ab, Bb, Cb, L):\n",
    "        return torch.stack([(Cb @ torch.matrix_power(Ab, l) @ Bb).squeeze() for l in range(L)]\n",
    "    )\n",
    "\n",
    "K2 = K_conv(Ab, Bb, Cb, L=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ae47992-8c74-4ba7-bf0d-be6491ca880b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1206+5.0381e-02j, -0.0295+4.9775e-03j,  0.0140-1.2264e-02j,\n",
       "         0.0300-1.4495e-02j,  0.0314-9.9820e-03j,  0.0259-3.6309e-03j,\n",
       "         0.0184+1.8998e-03j,  0.0111+5.4070e-03j,  0.0052+6.5853e-03j,\n",
       "         0.0010+5.6423e-03j, -0.0016+3.0413e-03j, -0.0032-6.6516e-04j,\n",
       "        -0.0039-4.9371e-03j, -0.0044-9.3039e-03j, -0.0047-1.3391e-02j,\n",
       "        -0.0053-1.6927e-02j, -0.0061-1.9739e-02j, -0.0073-2.1739e-02j,\n",
       "        -0.0088-2.2910e-02j, -0.0106-2.3289e-02j, -0.0127-2.2955e-02j,\n",
       "        -0.0148-2.2010e-02j, -0.0169-2.0571e-02j, -0.0188-1.8759e-02j,\n",
       "        -0.0206-1.6694e-02j, -0.0220-1.4486e-02j, -0.0231-1.2234e-02j,\n",
       "        -0.0238-1.0022e-02j, -0.0241-7.9199e-03j, -0.0239-5.9814e-03j,\n",
       "        -0.0233-4.2460e-03j, -0.0223-2.7393e-03j, -0.0209-1.4747e-03j,\n",
       "        -0.0191-4.5492e-04j, -0.0171+3.2614e-04j, -0.0148+8.8182e-04j,\n",
       "        -0.0123+1.2310e-03j, -0.0097+1.3965e-03j, -0.0069+1.4040e-03j,\n",
       "        -0.0042+1.2803e-03j, -0.0014+1.0528e-03j,  0.0014+7.4817e-04j,\n",
       "         0.0040+3.9204e-04j,  0.0065+8.1719e-06j,  0.0089-3.8186e-04j,\n",
       "         0.0111-7.5895e-04j,  0.0131-1.1066e-03j,  0.0149-1.4112e-03j,\n",
       "         0.0165-1.6616e-03j,  0.0179-1.8494e-03j,  0.0190-1.9687e-03j,\n",
       "         0.0200-2.0160e-03j,  0.0207-1.9898e-03j,  0.0212-1.8907e-03j,\n",
       "         0.0214-1.7207e-03j,  0.0215-1.4834e-03j,  0.0215-1.1837e-03j,\n",
       "         0.0212-8.2734e-04j,  0.0208-4.2090e-04j,  0.0203+2.8520e-05j,\n",
       "         0.0196+5.1344e-04j,  0.0188+1.0262e-03j,  0.0179+1.5590e-03j,\n",
       "         0.0170+2.1042e-03j,  0.0159+2.6544e-03j,  0.0149+3.2024e-03j,\n",
       "         0.0137+3.7416e-03j,  0.0126+4.2655e-03j,  0.0114+4.7685e-03j,\n",
       "         0.0102+5.2452e-03j,  0.0091+5.6911e-03j,  0.0079+6.1019e-03j,\n",
       "         0.0067+6.4743e-03j,  0.0056+6.8053e-03j,  0.0045+7.0925e-03j,\n",
       "         0.0035+7.3343e-03j,  0.0024+7.5293e-03j,  0.0014+7.6768e-03j,\n",
       "         0.0005+7.7766e-03j, -0.0004+7.8288e-03j, -0.0012+7.8340e-03j,\n",
       "        -0.0020+7.7931e-03j, -0.0027+7.7075e-03j, -0.0034+7.5787e-03j,\n",
       "        -0.0041+7.4086e-03j, -0.0046+7.1991e-03j, -0.0052+6.9526e-03j,\n",
       "        -0.0056+6.6715e-03j, -0.0061+6.3582e-03j, -0.0064+6.0154e-03j,\n",
       "        -0.0068+5.6457e-03j, -0.0071+5.2520e-03j, -0.0073+4.8368e-03j,\n",
       "        -0.0075+4.4030e-03j, -0.0077+3.9532e-03j, -0.0079+3.4901e-03j,\n",
       "        -0.0080+3.0162e-03j, -0.0080+2.5342e-03j, -0.0081+2.0465e-03j,\n",
       "        -0.0081+1.5553e-03j], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36d8ba-ccb3-4a80-beb8-312f591b18d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
