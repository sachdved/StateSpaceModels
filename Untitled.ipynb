{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd51126-661b-4d78-a05b-72ff992fe52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56804725-699f-415c-be3d-112a3b5c6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Layer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient layer for S4Ms. (Structured State Space Sequence Models).\n",
    "    Implements initialization of A as a NPLR matrix, enabling fast \n",
    "    matrix vector multiplication. \n",
    "\n",
    "    Several parameters, such as the projection matrix, are learned.\n",
    "\n",
    "    In this case, the C matrix is actually learned as C(1-A^L). \n",
    "    This is fairly easy to undo, and is done in the calc of Cbar.\n",
    "\n",
    "    Parameters:\n",
    "        N_input : dimension of input,\n",
    "        latent_dim : int, dimensions of latent space,\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        N_input : int,\n",
    "        latent_dim : int,\n",
    "        dt_min  : torch.Tensor = torch.tensor(0.001),\n",
    "        dt_max  : torch.Tensor = torch.tensor(0.1),\n",
    "        step_grad : bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert N_input==1\n",
    "\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.Lambda, self.P, self.B, _ = self.make_DPLR_HiPPO(self.latent_dim)\n",
    "        \n",
    "        self.Lambda = torch.autograd.Variable(self.Lambda, requires_grad = True)\n",
    "        self.P = torch.autograd.Variable(self.P, requires_grad = True)\n",
    "        self.B = torch.autograd.Variable(self.B, requires_grad = True)\n",
    "        \n",
    "        self.dt = torch.exp(self.log_step_initializer(dt_min, dt_max, step_grad))\n",
    "        \n",
    "        Ctilde = torch.nn.init.normal_(torch.empty(self.latent_dim, 2), mean=0, std=0.5**0.5)\n",
    "        self.Ctilde = torch.autograd.Variable(Ctilde[:,0] + Ctilde[:,1]*1j, requires_grad=True)\n",
    "\n",
    "        self.D = torch.autograd.Variable(torch.tensor(1.), requires_grad = True)\n",
    "\n",
    "    def make_HiPPO(self, N : int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        creates HiPPO matrix for legendre polynomials up to order N\n",
    "        parameters:\n",
    "            N: int\n",
    "        \"\"\"\n",
    "        P = torch.sqrt(1+2*torch.arange(N))\n",
    "        A = P.unsqueeze(1) * P.unsqueeze(0)\n",
    "        A = torch.tril(A) - torch.diag(torch.arange(N))\n",
    "        return -A\n",
    "        \n",
    "    def make_NPLR_HiPPO(self, N : int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        creating hippo matrix and associated low rank additive component, P\n",
    "        and the B matrix associated, as hippo forces it\n",
    "    \n",
    "        parameters:\n",
    "            N : int, degree of legendre polynomial coefficient\n",
    "        \"\"\"\n",
    "        nhippo = self.make_HiPPO(N)\n",
    "    \n",
    "        P = torch.sqrt(torch.arange(N)+0.5).to(torch.complex64)\n",
    "        B = torch.sqrt(2*torch.arange(N)+1.0).to(torch.complex64)\n",
    "    \n",
    "        return nhippo.to(torch.complex64), P, B\n",
    "\n",
    "    def make_DPLR_HiPPO(self, N : int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        convert matrices to DPLR representation\n",
    "        parameters:\n",
    "            N : int, degree of legendre polynomials\n",
    "        \"\"\"\n",
    "        A, P, B = self.make_NPLR_HiPPO(N)\n",
    "    \n",
    "        S = A + torch.outer(P, P)\n",
    "    \n",
    "        S_diag = torch.diagonal(S)\n",
    "        Lambda_real = torch.mean(S_diag) * torch.ones_like(S_diag)\n",
    "    \n",
    "        Lambda_imag, V = torch.linalg.eigh(S * -1j)\n",
    "        P = V.T.conj() @ P\n",
    "        B = V.T.conj() @ B\n",
    "        return Lambda_real + 1j * Lambda_imag, P, B, V\n",
    "\n",
    "    \n",
    "    def log_step_initializer(self, dt_min = torch.tensor(0.001), dt_max = torch.tensor(0.1), requires_grad = True):\n",
    "        \"\"\"\n",
    "        initial guess for dt, from random number generator. to be learned.\n",
    "    \n",
    "        parameters:\n",
    "            dt_min\n",
    "            dt_max\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.rand(1) * (torch.log(dt_max) - torch.log(dt_min)) + torch.log(dt_min), requires_grad = requires_grad)\n",
    "\n",
    "    \n",
    "    def K_gen_DPLR(\n",
    "        self,\n",
    "        Lambda : torch.Tensor, \n",
    "        P : torch.Tensor, \n",
    "        Q : torch.Tensor, \n",
    "        B: torch.Tensor, \n",
    "        C : torch.Tensor, \n",
    "        delta : torch.Tensor, \n",
    "        L : int\n",
    "    )-> torch.Tensor:\n",
    "        \"\"\"\n",
    "        computes convolution kernel from generating function using DPLR representation and\n",
    "        the cauchy kernel\n",
    "    \n",
    "        Parameters:\n",
    "            Lambda : diagonal part of DPLR\n",
    "            P : N matrix, rank 1 representation to A\n",
    "            Q : N matrix, rank 1 representation to A\n",
    "            C : N matrix, projection from latent to input\n",
    "            B : N matrix, projection from input to latent\n",
    "        \"\"\"\n",
    "        Omega_L = torch.exp(-2j*torch.pi * (torch.arange(L))/L)\n",
    "    \n",
    "        aterm = (torch.conj(C), torch.conj(Q))\n",
    "        bterm = (B, P)\n",
    "    \n",
    "        g = (2.0/delta) * ((1.0-Omega_L)/(1.0+Omega_L))\n",
    "        c = 2.0 / (1.0+Omega_L)\n",
    "    \n",
    "        k00 = self.cauchy(aterm[0] * bterm[0].unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "        k01 = self.cauchy(aterm[0] * bterm[1].unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "        k10 = self.cauchy(aterm[1] * bterm[0].unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "        k11 = self.cauchy(aterm[1] * bterm[1].unsqueeze(0), g.unsqueeze(1), Lambda)\n",
    "    \n",
    "        atRoots = c * (k00 - k01 * (1.0 / (1.0 + k11)) * k10)\n",
    "        out = torch.fft.irfft(atRoots, L)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def cauchy(self, k : torch.Tensor, omega : torch.Tensor, lambd : torch.Tensor):\n",
    "        \"\"\"\n",
    "        computes cauchy kernel \n",
    "        sum(c_i * b_i/(z - lambda_i)\n",
    "\n",
    "        Parameters:\n",
    "            k : term by term dot product of vectors\n",
    "            omega : function of the roots of unity\n",
    "            lambd: diagonal parts of the DPLR matrix\n",
    "        \"\"\"\n",
    "        return torch.sum(k/(omega-lambd), axis=1)\n",
    "\n",
    "    def discrete_DPLR(\n",
    "        self,\n",
    "        Lambda : torch.Tensor,\n",
    "        P : torch.Tensor,\n",
    "        Q : torch.Tensor,\n",
    "        B : torch.Tensor,\n",
    "        C : torch.Tensor,\n",
    "        delta : torch.Tensor,\n",
    "        L : int\n",
    "    )->(torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        computes the discretized version of the state space model,\n",
    "        assuming the DPLR form\n",
    "    \n",
    "        Parameters:\n",
    "            Lambda : Nx1, represents the diagonal values of the A matrix\n",
    "            P : Nx1, represents part of the low rank aspect of the A matrix\n",
    "            Q : Nx1, represents the other part of the low rank aspect of the A matrix\n",
    "            B : N, projection from input to latent\n",
    "            C : N, projection from latent to input\n",
    "            delta : step size\n",
    "            L : length of window\n",
    "        \"\"\"\n",
    "        Bt = B.unsqueeze(1)\n",
    "        Ct = C.unsqueeze(0)\n",
    "    \n",
    "        A = (torch.diag(Lambda) - torch.outer(P, torch.conj(Q)))\n",
    "        A0 = 2.0/delta * torch.eye(A.shape[0]) + A\n",
    "    \n",
    "        Qdagger = torch.conj(torch.transpose(Q))\n",
    "        \n",
    "        D = torch.diag(1.0/(2.0/delta - Lambda))\n",
    "        A1 = (D -  (1.0/(1.0 + Qdagger @ D @ P)) * D@P@Qdagger@D)\n",
    "        Ab = A1@A0\n",
    "        Bb = 2 * A1\n",
    "        Cb = Ct @ torch.conj(torch.linalg.inv(torch.eye(A.shape[0]) - torch.matrix_power(Ab, L)))\n",
    "        return Ab, Bb, Cb.conj()\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25f554b2-93c1-4d91-9f01-59a74468bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = S4Layer(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f1e00eb-5669-42a9-8c66-fba66f87cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bloop = test.K_gen_DPLR(test.Lambda, test.P, test.P, test.B, test.Ctilde, test.dt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f40b9bf7-be66-484d-9702-09d2006d92da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Ctilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f892f64-394f-477f-a3be-d0ba71ad8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_DPLR(Lambda, P, Q, B, C, step, L):\n",
    "    # Convert parameters to matrices\n",
    "    B = B.unsqueeze(1)\n",
    "    Ct = C.unsqueeze(0)\n",
    "\n",
    "    N = Lambda.shape[0]\n",
    "    A = torch.diag(Lambda) - torch.outer(P, Q.conj())\n",
    "    I = torch.eye(N)\n",
    "\n",
    "    # Forward Euler\n",
    "    A0 = (2.0 / step) * I + A\n",
    "\n",
    "    # Backward Euler\n",
    "    D = torch.diag(1.0 / ((2.0 / step) - Lambda))\n",
    "    Qc = Q.conj().T.reshape(1, -1)\n",
    "    P2 = P.reshape(-1, 1)\n",
    "    A1 = D - (D @ P2 * (1.0 / (1 + (Qc @ D @ P2))) * Qc @ D)\n",
    "\n",
    "    # A bar and B bar\n",
    "    Ab = A1 @ A0\n",
    "    Bb = 2 * A1 @ B\n",
    "\n",
    "    # Recover Cbar from Ct\n",
    "    Cb = Ct @ torch.linalg.inv(I - torch.matrix_power(Ab, L)).conj()\n",
    "    return Ab, Bb, Cb.conj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8bebd0c-22d7-46fa-85b4-77d9520813e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ab, Bb, Cb = discrete_DPLR(test.Lambda, test.P, test.P, test.B, test.Ctilde, test.dt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c20c22fe-1dc8-44a7-b281-cb3c7a3d898b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0012-0.0018j, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloop[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1b3af93-8a88-4e8a-8e45-becf529d059e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0012-0.0018j]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cb@torch.matrix_power(Ab, 2)@Bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e2156f7-9435-4288-ab24-42f4c34d417a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type double without overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mK_conv\u001b[39m(Ab, Bb, Cb, L):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor(\n\u001b[1;32m      3\u001b[0m         [(Cb \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatrix_power(Ab, l) \u001b[38;5;241m@\u001b[39m Bb)\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(L)]\n\u001b[1;32m      4\u001b[0m     )\n\u001b[0;32m----> 5\u001b[0m K2 \u001b[38;5;241m=\u001b[39m \u001b[43mK_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m, in \u001b[0;36mK_conv\u001b[0;34m(Ab, Bb, Cb, L)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mK_conv\u001b[39m(Ab, Bb, Cb, L):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_power\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type double without overflow"
     ]
    }
   ],
   "source": [
    "def K_conv(Ab, Bb, Cb, L):\n",
    "    return torch.Tensor(\n",
    "        [(Cb @ torch.matrix_power(Ab, l) @ Bb).squeeze() for l in range(L)]\n",
    "    )\n",
    "K2 = K_conv(Ab, Bb, Cb, L=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae47992-8c74-4ba7-bf0d-be6491ca880b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
