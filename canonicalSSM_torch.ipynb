{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baaff332-22f6-4ed0-a94f-9d19b068770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed1d768-e46f-4911-9e30-9d6091aded87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_SSM(N : int) -> (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "    A = torch.autograd.Variable(torch.rand(size=(N,N)), requires_grad = True)\n",
    "    B = torch.autograd.Variable(torch.rand(size=(N,1)), requires_grad = True)\n",
    "    C = torch.autograd.Variable(torch.rand(size=(1,N)), requires_grad = True)\n",
    "    D = torch.autograd.Variable(torch.rand(size=(1,1)), requires_grad = True)\n",
    "    return A, B, C, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c913ebed-b17c-4993-a4c4-be749db92f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, C, D = random_SSM(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e5a72b-292c-43a7-bda5-6edbf96b7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.zeros((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60b605e-0171-4aec-acd1-793ea3007d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = torch.tensor(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53248f90-d07b-45a7-b111-d45f5b2f6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(\n",
    "    A : torch.Tensor, B : torch.Tensor, C : torch.Tensor, D : torch.Tensor, delta : torch.Tensor\n",
    ") -> (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "    \"\"\"Discretizes SSM using bilinear model\n",
    "\n",
    "    parameters:\n",
    "        A: (NxN) transition matrix in latent\n",
    "        B: (Nx1) projection matrix to latent\n",
    "        C: (1xN) projection matrix from latent to output\n",
    "        D: (1x1) skip connection from input to output\n",
    "        delta: time step, ensure sufficient smallness\n",
    "    \"\"\"\n",
    "    Cbar = C\n",
    "    Dbar = D\n",
    "    N = A.shape[0]\n",
    "    Bl = torch.linalg.inv(torch.eye(N) - delta / 2 * A)\n",
    "    Abar = Bl@(torch.eye(N) + delta/2 * A)\n",
    "    Bbar = Bl@(delta*B)\n",
    "    return Abar, Bbar, Cbar, Dbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3305b7a1-5b3e-4abc-8d77-f34f8a399075",
   "metadata": {},
   "outputs": [],
   "source": [
    "Abar, Bbar, Cbar, Dbar = discretize(A, B, C, D, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72635d9-919f-4482-a03c-c6a364503b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "num_steps = int(T/delta)\n",
    "\n",
    "u = torch.cos(torch.arange(num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3134525-8e1e-40be-b02b-346dcbb7bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_SSM(\n",
    "    Ab : torch.Tensor, Bb : torch.Tensor, Cb : torch.Tensor, Db : torch.Tensor,  u : torch.Tensor, x0 : torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    computes steps of the SSM going forward.\n",
    "\n",
    "    parameters:\n",
    "        Ab : (NxN) transition matrix in discrete space of latent to latent\n",
    "        Bb : (Nx1) projcetion matrix from input to latent space\n",
    "        Cb : (1xN) projection matrix from latent to output\n",
    "        Db : (1x1) skip connection input to output\n",
    "        u  : (L,)  trajectory we are trying to track\n",
    "        x0 : (Nx1) initial condition of latent\n",
    "    \"\"\"\n",
    "    x0 = torch.zeros((10,1))\n",
    "    x = torch.zeros((Ab.shape[0], len(u[:100])))\n",
    "    y = torch.zeros_like(u[:100])\n",
    "    for i in range(u[:100].shape[0]):\n",
    "        x[:,i] = (Ab@x0 + Bb*u[i]).squeeze()\n",
    "        y[i] = (Cb@x[:,i]).squeeze()\n",
    "        x0 = x[:,i].unsqueeze(-1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "397018e5-477a-4464-875d-00e7156dcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_conv(Ab : torch.Tensor, Bb : torch.Tensor, Cb : torch.Tensor, L : int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    computes convolution window given L time steps using equation K_t = Cb @ (Ab^t) @ Bb. \n",
    "    Needs to be flipped for correct causal convolution, but can be used as is in fft mode\n",
    "\n",
    "    parameters:\n",
    "        Ab : transition matrix\n",
    "        Bb : projection matrix from input to latent\n",
    "        Cb : projection matrix from latent to input\n",
    "        Db : skip connection\n",
    "        L  : length over which we want convolutional window\n",
    "    \"\"\"\n",
    "    return torch.stack([(Cb @ torch.matrix_power(Ab, l) @ Bb).squeeze() for l in range(L)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4daebfc-d2d8-41b9-8599-4ddd690b3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_conv(u : torch.Tensor, K : torch.Tensor, notfft : bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    computes 1-d causal convolution either using standard method or fft transform.\n",
    "\n",
    "    parameters:\n",
    "        u : trajectory to convolve\n",
    "        K : convolutional filter\n",
    "        notfft: boolean, for whether or not we use fft mode or not.\n",
    "    \"\"\"\n",
    "    assert len(u.shape)==1\n",
    "    assert K.shape==u.shape\n",
    "    \n",
    "    L = u.shape[0]\n",
    "    powers_of_2 = 2**int(math.ceil(math.log2(2*L)))\n",
    "\n",
    "    if notfft:\n",
    "        padded_u = torch.nn.functional.pad(u, (L-1,L-1))\n",
    "        convolve = torch.zeros_like(u)\n",
    "        for i in range(L):\n",
    "            convolve[i] = torch.sum(padded_u[i:i+L]*K.flip(dims=[0]))\n",
    "        return convolve\n",
    "    else:\n",
    "\n",
    "        K_pad = torch.nn.functional.pad(K, (0, L))\n",
    "        u_pad = torch.nn.functional.pad(u, (0, L))\n",
    "        \n",
    "        K_f, u_f = torch.fft.rfft(K_pad, n = powers_of_2), torch.fft.rfft(u_pad, n = powers_of_2)\n",
    "        return torch.fft.irfft(K_f * u_f, n = powers_of_2)[:L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d3efcc-e3f7-43ca-bda3-7601774fd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = K_conv(Abar, Bbar, Cbar, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f74b795-1716-45f9-8c7e-3be76d1a88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_fft = causal_conv(u[:100], K)\n",
    "conv_notfft = causal_conv(u[:100],K , notfft=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f4895f0-52c5-444c-9837-f90d033de1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = scan_SSM(Abar, Bbar, Cbar, Dbar, u[:100], torch.zeros((10,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa14979-dfe1-41e2-b0ae-9834511822a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print((abs(conv_fft - conv_notfft)<1e-5).all())\n",
    "print((abs(conv_fft - y)<1e-5).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8c56ba-ea9b-4ed5-bb0a-82539bd529b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_step_initializer(dt_min = 0.001, dt_max = 0.1):\n",
    "    \"\"\"\n",
    "    initial guess for dt, from random number generator. to be learned.\n",
    "\n",
    "    parameters:\n",
    "        dt_min\n",
    "        dt_max\n",
    "    \"\"\"\n",
    "    return torch.autograd.Variable(torch.rand(1) * (torch.log(dt_max) - torch.log(dt_min)) + torch.log(dt_min), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47782377-fad9-4f35-a7c5-4c12a7ffbdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSMLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple layer that does SSMing. Assumes single input, single output. \n",
    "    Could be made multi-dimensional either by stacking and decorrelating,\n",
    "    or by playing with the code to allow for multi input, multioutput. Should be relatively easy, \n",
    "    but need to carefully think a little about convolution of multi dim inputs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim,\n",
    "        dt_min = 0.001,\n",
    "        dt_max = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.A, self.B, self.C, self.D = self.random_SSM(latent_dim)\n",
    "        self.dt = self.log_step_initializer(dt_min, dt_max)\n",
    "        self.Abar, self.Bbar, self.Cbar, self.Dbar = self.discretize(self.A, self.B, self.C, self.D, self.dt)\n",
    "\n",
    "\n",
    "    def random_SSM(\n",
    "        self, \n",
    "        N : int\n",
    "    ) -> (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        initializing SSM parameters given latent dim\n",
    "        \n",
    "        parameters:\n",
    "            N : size of latent dimension\n",
    "        \"\"\"\n",
    "        A = torch.autograd.Variable(torch.rand(size=(N,N)), requires_grad = True)\n",
    "        B = torch.autograd.Variable(torch.rand(size=(N,1)), requires_grad = True)\n",
    "        C = torch.autograd.Variable(torch.rand(size=(1,N)), requires_grad = True)\n",
    "        D = torch.autograd.Variable(torch.rand(size=(1,1)), requires_grad = True)\n",
    "        return A, B, C, D\n",
    "\n",
    "    def log_step_initializer(self, dt_min = 0.001, dt_max = 0.1):\n",
    "        \"\"\"\n",
    "        initial guess for dt, from random number generator. to be learned.\n",
    "    \n",
    "        parameters:\n",
    "            dt_min\n",
    "            dt_max\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.rand(1) * (torch.log(torch.tensor(dt_max)) - torch.log(torch.tensor(dt_min))) + torch.log(torch.tensor(dt_min)), requires_grad = True)\n",
    "\n",
    "    def discretize(\n",
    "        self, A : torch.Tensor, B : torch.Tensor, C : torch.Tensor, D : torch.Tensor, delta : torch.Tensor\n",
    "    ) -> (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"Discretizes SSM using bilinear model\n",
    "    \n",
    "        parameters:\n",
    "            A: (NxN) transition matrix in latent\n",
    "            B: (Nx1) projection matrix to latent\n",
    "            C: (1xN) projection matrix from latent to output\n",
    "            D: (1x1) skip connection from input to output\n",
    "            delta: time step, ensure sufficient smallness\n",
    "        \"\"\"\n",
    "        Cbar = C\n",
    "        Dbar = D\n",
    "        N = A.shape[0]\n",
    "        Bl = torch.linalg.inv(torch.eye(N) - delta / 2 * A)\n",
    "        Abar = Bl@(torch.eye(N) + delta/2 * A)\n",
    "        Bbar = Bl@(delta*B)\n",
    "        return Abar, Bbar, Cbar, Dbar\n",
    "\n",
    "    def scan_SSM(\n",
    "        self, Ab : torch.Tensor, Bb : torch.Tensor, Cb : torch.Tensor, Db : torch.Tensor,  u : torch.Tensor, x0 : torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        computes steps of the SSM going forward.\n",
    "    \n",
    "        parameters:\n",
    "            Ab : (NxN) transition matrix in discrete space of latent to latent\n",
    "            Bb : (Nx1) projcetion matrix from input to latent space\n",
    "            Cb : (1xN) projection matrix from latent to output\n",
    "            Db : (1x1) skip connection input to output\n",
    "            u  : (L,)  trajectory we are trying to track\n",
    "            x0 : (Nx1) initial condition of latent\n",
    "        \"\"\"\n",
    "        x0 = torch.zeros((10,1))\n",
    "        x = torch.zeros((Ab.shape[0], len(u)))\n",
    "        y = torch.zeros_like(u)\n",
    "        for i in range(u.shape[0]):\n",
    "            x[:,i] = (Ab@x0 + Bb*u[i]).squeeze()\n",
    "            y[i] = (Cb@x[:,i]).squeeze()\n",
    "            x0 = x[:,i].unsqueeze(-1)\n",
    "        return x, y\n",
    "        \n",
    "    def K_conv(self, Ab : torch.Tensor, Bb : torch.Tensor, Cb : torch.Tensor, L : int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        computes convolution window given L time steps using equation K_t = Cb @ (Ab^t) @ Bb. \n",
    "        Needs to be flipped for correct causal convolution, but can be used as is in fft mode\n",
    "    \n",
    "        parameters:\n",
    "            Ab : transition matrix\n",
    "            Bb : projection matrix from input to latent\n",
    "            Cb : projection matrix from latent to input\n",
    "            Db : skip connection\n",
    "            L  : length over which we want convolutional window\n",
    "        \"\"\"\n",
    "        return torch.stack([(Cb @ torch.matrix_power(Ab, l) @ Bb).squeeze() for l in range(L)])\n",
    "\n",
    "    def causal_conv(u : torch.Tensor, K : torch.Tensor, notfft : bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        computes 1-d causal convolution either using standard method or fft transform.\n",
    "    \n",
    "        parameters:\n",
    "            u : trajectory to convolve\n",
    "            K : convolutional filter\n",
    "            notfft: boolean, for whether or not we use fft mode or not.\n",
    "        \"\"\"\n",
    "        assert K.shape==u.shape\n",
    "        \n",
    "        L = u.shape[0]\n",
    "        powers_of_2 = 2**int(math.ceil(math.log2(2*L)))\n",
    "    \n",
    "        if notfft:\n",
    "            padded_u = torch.nn.functional.pad(u, (L-1,L-1))\n",
    "            convolve = torch.zeros_like(u)\n",
    "            for i in range(L):\n",
    "                convolve[i] = torch.sum(padded_u[i:i+L]*K.flip(dims=[0]))\n",
    "            return convolve\n",
    "        else:\n",
    "    \n",
    "            K_pad = torch.nn.functional.pad(K, (0, L))\n",
    "            u_pad = torch.nn.functional.pad(u, (0, L))\n",
    "            \n",
    "            K_f, u_f = torch.fft.rfft(K_pad, n = powers_of_2), torch.fft.rfft(u_pad, n = powers_of_2)\n",
    "            return torch.fft.irfft(K_f * u_f, n = powers_of_2)[:L]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        u : torch.Tensor,\n",
    "        x0 : torch.Tensor = torch.zeros((1,1)),\n",
    "        mode : bool | str = False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        forward pass of model\n",
    "\n",
    "        Parameters:\n",
    "            u  : input time series\n",
    "            x0 : initial condition, only used in recurrent mode\n",
    "            mode: recurrent mode (\"recurrent\"), or convolution mode (True : direct convolution, False : fourier transform)\n",
    "        \"\"\"\n",
    "        if mode == \"recurrent\":\n",
    "            return self.scan_SSM(self.Abar, self.Bbar, self.Cbar, u, x0)[1]\n",
    "        else:\n",
    "            K = self.K_conv(self.Abar, self.Bbar, self.Cbar, u.shape[0])\n",
    "            return self.causal_conv(u, K, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5306197b-fa32-44f1-a7b3-6a2c61c0675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_HiPPO(N : int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    creates HiPPO matrix for legendre polynomials up to order N\n",
    "    parameters:\n",
    "        N: int\n",
    "    \"\"\"\n",
    "    P = torch.sqrt(1+2*torch.arange(N))\n",
    "    A = P.unsqueeze(1) * P.unsqueeze(0)\n",
    "    A = torch.tril(A) - torch.diag(torch.arange(N))\n",
    "    return -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71aa99a8-a247-4d94-b7c3-93ac93bbbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_gen_inverse(\n",
    "    Abar : torch.Tensor, Bbar : torch.Tensor, Cbar : torch.Tensor, L : int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    creates generating function for convolutional window, to be evaluated at roots of unity\n",
    "    parameters:\n",
    "        Abar : discretized A matrix\n",
    "        Bbar : discretized B matrix\n",
    "        Cbar : discretized C matrix\n",
    "        L    : length of convolutional window\n",
    "    \"\"\"\n",
    "    Abar = Abar.to(torch.complex64)\n",
    "    Bbar = Bbar.to(torch.complex64)\n",
    "    Cbar = Cbar.to(torch.complex64)\n",
    "    \n",
    "    I = torch.eye(Abar.shape[0]).to(torch.complex64)\n",
    "    Al = torch.matrix_power(Abar, L)\n",
    "    Ctilde = Cbar @ (I - (Al))\n",
    "    return lambda z: (torch.conj(Ctilde)@(torch.linalg.inv(I-(Abar * z)))@Bbar).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddce6900-8991-4cf9-ac63-9b2159051b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_from_gen(gen : Callable, L : int):\n",
    "    \"\"\"\n",
    "    returns convolution from generating function by evaluating at roots of unity\n",
    "\n",
    "    parameters:\n",
    "        gen : generating function\n",
    "        L   : int\n",
    "    \"\"\"\n",
    "    omega_L = torch.exp(-2j * torch.pi * torch.arange(L)/L)\n",
    "    atRoots = torch.tensor([gen(omega) for omega in omega_L])\n",
    "    return torch.fft.irfft(atRoots, L).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4606f8b-22a8-479b-abbf-0ef619b4e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy(v : torch.Tensor, omega : torch.Tensor, lambd : torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    helper function for calculating cauchy kernel in generating function\n",
    "\n",
    "    parameters:\n",
    "        v : a dot product vector, relying on DPLR representation of SSM matrices\n",
    "        omega : complex poles\n",
    "        lambd : diagonal values of A matrix stand-in\n",
    "    \"\"\"\n",
    "    cauchy_dot = lambda _omega: (v/(omega-lamb)).sum()\n",
    "    return cauchy_dot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20fe8246-72ca-480e-9144-81e079776339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_gen_DPLR(\n",
    "    Lambda : torch.Tensor, \n",
    "    P : torch.Tensor, \n",
    "    Q : torch.Tensor, \n",
    "    B: torch.Tensor, \n",
    "    C : torch.Tensor, \n",
    "    delta : torch.Tensor, \n",
    "    L : int\n",
    ")-> torch.Tensor:\n",
    "    \"\"\"\n",
    "    computes convolution kernel from generating function using DPLR representation and\n",
    "    the cauchy kernel\n",
    "\n",
    "    Parameters:\n",
    "        Lambda : diagonal part of A\n",
    "        P : Nx1 matrix, rank 1 representation to A\n",
    "        Q : Nx1 matrix, rank 1 representation to A\n",
    "        C : 1xN matrix, projection from latent to input\n",
    "        B : Nx1 matrix, projection from input to latent\n",
    "    \"\"\"\n",
    "    Omega_L = torch.exp(-2j*torch.pi * (torch.arange(L))/L)\n",
    "\n",
    "    aterm = (torch.conj(C), torch.conj(Q))\n",
    "    bterm = (B, P)\n",
    "\n",
    "    g = (2.0/delta) * ((1.0-Omega_L)/(1.0+Omega_L))\n",
    "    c = 2.0 / (1.0+Omega_L)\n",
    "\n",
    "    k00 = cauchy(aterm[0] * bterm[0], g, Lambda)\n",
    "    k01 = cauchy(aterm[0] * bterm[1], g, Lambda)\n",
    "    k10 = cauchy(aterm[1] * bterm[0], g, Lambda)\n",
    "    k11 = cauchy(aterm[1] * bterm[1], g, Lambda)\n",
    "\n",
    "    atRoots = c * (k00 - k01 * (1.0 / (1.0 + k11)) * k10)\n",
    "    out = np.fft.irfft(atRoots, L)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b2a6a93-3bb6-401e-8a7f-94492174f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_DPLR(N):\n",
    "    Lambda = torch.rand((N,))\n",
    "    P = torch.rand((N,))\n",
    "    Q = torch.rand((N,))\n",
    "    B = torch.rand((N, 1))\n",
    "    C = torch.rand((1, N))\n",
    "    return Lambda, P, Q, B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9651a2c6-25d4-4a7b-b7ca-85f81feb3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, P, Q, B, C = random_DPLR(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da8b16f7-0c97-45a4-b63e-ebc54c302520",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mK_gen_DPLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 29\u001b[0m, in \u001b[0;36mK_gen_DPLR\u001b[0;34m(Lambda, P, Q, B, C, delta, L)\u001b[0m\n\u001b[1;32m     26\u001b[0m g \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m/\u001b[39mdelta) \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m-\u001b[39mOmega_L)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m+\u001b[39mOmega_L))\n\u001b[1;32m     27\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m+\u001b[39mOmega_L)\n\u001b[0;32m---> 29\u001b[0m k00 \u001b[38;5;241m=\u001b[39m cauchy(\u001b[43materm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbterm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, g, Lambda)\n\u001b[1;32m     30\u001b[0m k01 \u001b[38;5;241m=\u001b[39m cauchy(aterm[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m bterm[\u001b[38;5;241m1\u001b[39m], g, Lambda)\n\u001b[1;32m     31\u001b[0m k10 \u001b[38;5;241m=\u001b[39m cauchy(aterm[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m bterm[\u001b[38;5;241m0\u001b[39m], g, Lambda)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "K_gen_DPLR(Lambda, P, P, B, C, delta, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fee3dce0-f254-4b94-869b-dc4b514a9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega_L = torch.exp(-2j*torch.pi * (torch.arange(100))/100)\n",
    "\n",
    "aterm = (torch.conj(C).squeeze(), torch.conj(P).squeeze())\n",
    "bterm = (B.squeeze(), P.squeeze())\n",
    "\n",
    "g = (2.0/delta) * ((1.0-Omega_L)/(1.0+Omega_L))\n",
    "c = 2.0 / (1.0+Omega_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe40fc16-6920-4754-94f4-b3902620eaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5348+6.2853j, -0.0200+6.2853j, -0.1802+6.2853j, -0.3887+6.2853j,\n",
       "        -0.3949+6.2853j, -0.7301+6.2853j, -0.3963+6.2853j, -0.6580+6.2853j])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11ea6fff-2e1e-4c95-8693-31e38e86e33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(aterm[0] * bterm[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a2e30ed-664c-4e9f-a314-c32c87cf541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "term1 = (aterm[0] * bterm[0]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea3357e6-1f11-436c-95cd-a318aef21033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1802+6.2853j)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[1] - Lambda[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b98674c-f684-4336-a8a9-e1c8c549496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = g.unsqueeze(1) - Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abec02b0-9b6c-4483-923f-617f293f6c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a6fb105-c7e1-4c99-a596-617ec775d232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0458-0.j, -2.3463-0.j, -1.6375-0.j, -1.2626-0.j, -0.1732-0.j, -0.0760-0.j,\n",
       "        -0.2237-0.j, -0.2555-0.j])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1[0]/test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2ec0664-afb3-4ae8-ac42-c04caab76e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0458e+00-0.0000e+00j, -2.3463e+00-0.0000e+00j,\n",
       "         -1.6375e+00-0.0000e+00j, -1.2626e+00-0.0000e+00j,\n",
       "         -1.7315e-01-0.0000e+00j, -7.6041e-02-0.0000e+00j,\n",
       "         -2.2374e-01-0.0000e+00j, -2.5547e-01-0.0000e+00j],\n",
       "        [-7.5175e-03-8.8348e-02j, -2.3668e-05-7.4520e-03j,\n",
       "         -1.3453e-03-4.6915e-02j, -4.8113e-03-7.7792e-02j,\n",
       "         -6.8067e-04-1.0835e-02j, -1.0123e-03-8.7150e-03j,\n",
       "         -8.8588e-04-1.4051e-02j, -2.7696e-03-2.6455e-02j],\n",
       "        [-1.8858e-03-4.4370e-02j, -5.9060e-06-3.7223e-03j,\n",
       "         -3.3587e-04-2.3449e-02j, -1.2039e-03-3.8969e-02j,\n",
       "         -1.7033e-04-5.4281e-03j, -2.5512e-04-4.3971e-03j,\n",
       "         -2.2169e-04-7.0393e-03j, -6.9670e-04-1.3323e-02j],\n",
       "        [-8.3622e-04-2.9561e-02j, -2.6157e-06-2.4775e-03j,\n",
       "         -1.4880e-04-1.5609e-02j, -5.3358e-04-2.5950e-02j,\n",
       "         -7.5495e-05-3.6148e-03j, -1.1322e-04-2.9321e-03j,\n",
       "         -9.8259e-05-4.6877e-03j, -3.0909e-04-8.8809e-03j],\n",
       "        [-4.6837e-04-2.2127e-02j, -1.4648e-06-1.8538e-03j,\n",
       "         -8.3316e-05-1.1680e-02j, -2.9881e-04-1.9421e-02j,\n",
       "         -4.2278e-05-2.7053e-03j, -6.3436e-05-2.1954e-03j,\n",
       "         -5.5026e-05-3.5083e-03j, -1.7315e-04-6.6488e-03j],\n",
       "        [-2.9802e-04-1.7652e-02j, -9.3194e-07-1.4786e-03j,\n",
       "         -5.3006e-05-9.3162e-03j, -1.9011e-04-1.5492e-02j,\n",
       "         -2.6899e-05-2.1580e-03j, -4.0370e-05-1.7516e-03j,\n",
       "         -3.5010e-05-2.7986e-03j, -1.1019e-04-5.3045e-03j],\n",
       "        [-2.0546e-04-1.4657e-02j, -6.4238e-07-1.2277e-03j,\n",
       "         -3.6540e-05-7.7351e-03j, -1.3106e-04-1.2863e-02j,\n",
       "         -1.8544e-05-1.7918e-03j, -2.7834e-05-1.4546e-03j,\n",
       "         -2.4136e-05-2.3237e-03j, -7.5968e-05-4.4048e-03j],\n",
       "        [-1.4965e-04-1.2509e-02j, -4.6787e-07-1.0477e-03j,\n",
       "         -2.6613e-05-6.6013e-03j, -9.5458e-05-1.0978e-02j,\n",
       "         -1.3506e-05-1.5292e-03j, -2.0274e-05-1.2415e-03j,\n",
       "         -1.7579e-05-1.9831e-03j, -5.5333e-05-3.7594e-03j],\n",
       "        [-1.1342e-04-1.0891e-02j, -3.5458e-07-9.1211e-04j,\n",
       "         -2.0170e-05-5.7469e-03j, -7.2350e-05-9.5574e-03j,\n",
       "         -1.0237e-05-1.3313e-03j, -1.5367e-05-1.0809e-03j,\n",
       "         -1.3323e-05-1.7265e-03j, -4.1939e-05-3.2730e-03j],\n",
       "        [-8.8589e-05-9.6249e-03j, -2.7695e-07-8.0609e-04j,\n",
       "         -1.5754e-05-5.0789e-03j, -5.6508e-05-8.4466e-03j,\n",
       "         -7.9954e-06-1.1766e-03j, -1.2002e-05-9.5526e-04j,\n",
       "         -1.0406e-05-1.5258e-03j, -3.2757e-05-2.8927e-03j],\n",
       "        [-7.0829e-05-8.6063e-03j, -2.2142e-07-7.2076e-04j,\n",
       "         -1.2595e-05-4.5413e-03j, -4.5179e-05-7.5526e-03j,\n",
       "         -6.3924e-06-1.0520e-03j, -9.5962e-06-8.5418e-04j,\n",
       "         -8.3199e-06-1.3643e-03j, -2.6190e-05-2.5866e-03j],\n",
       "        [-5.7691e-05-7.7673e-03j, -1.8038e-07-6.5049e-04j,\n",
       "         -1.0259e-05-4.0986e-03j, -3.6799e-05-6.8163e-03j,\n",
       "         -5.2067e-06-9.4948e-04j, -7.8164e-06-7.7091e-04j,\n",
       "         -6.7767e-06-1.2313e-03j, -2.1333e-05-2.3344e-03j],\n",
       "        [-4.7702e-05-7.0629e-03j, -1.4912e-07-5.9150e-04j,\n",
       "         -8.4825e-06-3.7269e-03j, -3.0427e-05-6.1981e-03j,\n",
       "         -4.3052e-06-8.6338e-04j, -6.4631e-06-7.0101e-04j,\n",
       "         -5.6033e-06-1.1197e-03j, -1.7639e-05-2.1227e-03j],\n",
       "        [-3.9932e-05-6.4622e-03j, -1.2484e-07-5.4118e-04j,\n",
       "         -7.1008e-06-3.4099e-03j, -2.5471e-05-5.6709e-03j,\n",
       "         -3.6039e-06-7.8994e-04j, -5.4104e-06-6.4139e-04j,\n",
       "         -4.6906e-06-1.0244e-03j, -1.4766e-05-1.9422e-03j],\n",
       "        [-3.3770e-05-5.9428e-03j, -1.0556e-07-4.9768e-04j,\n",
       "         -6.0050e-06-3.1358e-03j, -2.1541e-05-5.2151e-03j,\n",
       "         -3.0478e-06-7.2644e-04j, -4.5755e-06-5.8984e-04j,\n",
       "         -3.9668e-06-9.4208e-04j, -1.2488e-05-1.7861e-03j],\n",
       "        [-2.8804e-05-5.4884e-03j, -9.0046e-08-4.5962e-04j,\n",
       "         -5.1219e-06-2.8960e-03j, -1.8372e-05-4.8163e-03j,\n",
       "         -2.5995e-06-6.7090e-04j, -3.9026e-06-5.4474e-04j,\n",
       "         -3.3834e-06-8.7004e-04j, -1.0651e-05-1.6495e-03j],\n",
       "        [-2.4742e-05-5.0868e-03j, -7.7331e-08-4.2599e-04j,\n",
       "         -4.3996e-06-2.6841e-03j, -1.5782e-05-4.4639e-03j,\n",
       "         -2.2330e-06-6.2180e-04j, -3.3523e-06-5.0488e-04j,\n",
       "         -2.9063e-06-8.0638e-04j, -9.1492e-06-1.5288e-03j],\n",
       "        [-2.1381e-05-4.7286e-03j, -6.6831e-08-3.9599e-04j,\n",
       "         -3.8018e-06-2.4951e-03j, -1.3638e-05-4.1496e-03j,\n",
       "         -1.9296e-06-5.7802e-04j, -2.8969e-06-4.6933e-04j,\n",
       "         -2.5114e-06-7.4960e-04j, -7.9061e-06-1.4212e-03j],\n",
       "        [-1.8568e-05-4.4066e-03j, -5.8056e-08-3.6902e-04j,\n",
       "         -3.3017e-06-2.3251e-03j, -1.1843e-05-3.8670e-03j,\n",
       "         -1.6757e-06-5.3866e-04j, -2.5158e-06-4.3737e-04j,\n",
       "         -2.1810e-06-6.9855e-04j, -6.8660e-06-1.3244e-03j],\n",
       "        [-1.6191e-05-4.1149e-03j, -5.0621e-08-3.4460e-04j,\n",
       "         -2.8791e-06-2.1712e-03j, -1.0328e-05-3.6110e-03j,\n",
       "         -1.4613e-06-5.0300e-04j, -2.1938e-06-4.0843e-04j,\n",
       "         -1.9019e-06-6.5231e-04j, -5.9872e-06-1.2367e-03j],\n",
       "        [-1.4167e-05-3.8491e-03j, -4.4289e-08-3.2233e-04j,\n",
       "         -2.5191e-06-2.0310e-03j, -9.0361e-06-3.3777e-03j,\n",
       "         -1.2785e-06-4.7050e-04j, -1.9194e-06-3.8204e-04j,\n",
       "         -1.6640e-06-6.1017e-04j, -5.2385e-06-1.1568e-03j],\n",
       "        [-1.2428e-05-3.6052e-03j, -3.8843e-08-3.0192e-04j,\n",
       "         -2.2100e-06-1.9023e-03j, -7.9274e-06-3.1638e-03j,\n",
       "         -1.1217e-06-4.4070e-04j, -1.6840e-06-3.5784e-04j,\n",
       "         -1.4599e-06-5.7152e-04j, -4.5958e-06-1.0836e-03j],\n",
       "        [-1.0927e-05-3.3804e-03j, -3.4151e-08-2.8309e-04j,\n",
       "         -1.9429e-06-1.7837e-03j, -6.9695e-06-2.9665e-03j,\n",
       "         -9.8612e-07-4.1322e-04j, -1.4805e-06-3.3552e-04j,\n",
       "         -1.2835e-06-5.3587e-04j, -4.0405e-06-1.0160e-03j],\n",
       "        [-9.6212e-06-3.1720e-03j, -3.0083e-08-2.6564e-04j,\n",
       "         -1.7108e-06-1.6737e-03j, -6.1369e-06-2.7836e-03j,\n",
       "         -8.6831e-07-3.8774e-04j, -1.3036e-06-3.1484e-04j,\n",
       "         -1.1301e-06-5.0284e-04j, -3.5577e-06-9.5336e-04j],\n",
       "        [-8.4801e-06-2.9780e-03j, -2.6510e-08-2.4939e-04j,\n",
       "         -1.5079e-06-1.5713e-03j, -5.4090e-06-2.6133e-03j,\n",
       "         -7.6532e-07-3.6403e-04j, -1.1490e-06-2.9558e-04j,\n",
       "         -9.9609e-07-4.7208e-04j, -3.1358e-06-8.9504e-04j],\n",
       "        [-7.4781e-06-2.7965e-03j, -2.3382e-08-2.3419e-04j,\n",
       "         -1.3298e-06-1.4756e-03j, -4.7699e-06-2.4541e-03j,\n",
       "         -6.7489e-07-3.4184e-04j, -1.0132e-06-2.7757e-04j,\n",
       "         -8.7839e-07-4.4331e-04j, -2.7653e-06-8.4050e-04j],\n",
       "        [-6.5943e-06-2.6261e-03j, -2.0605e-08-2.1992e-04j,\n",
       "         -1.1725e-06-1.3857e-03j, -4.2061e-06-2.3045e-03j,\n",
       "         -5.9513e-07-3.2101e-04j, -8.9348e-07-2.6066e-04j,\n",
       "         -7.7458e-07-4.1630e-04j, -2.4385e-06-7.8928e-04j],\n",
       "        [-5.8122e-06-2.4655e-03j, -1.8161e-08-2.0647e-04j,\n",
       "         -1.0335e-06-1.3009e-03j, -3.7073e-06-2.1636e-03j,\n",
       "         -5.2455e-07-3.0138e-04j, -7.8752e-07-2.4471e-04j,\n",
       "         -6.8271e-07-3.9083e-04j, -2.1493e-06-7.4100e-04j],\n",
       "        [-5.1179e-06-2.3135e-03j, -1.6004e-08-1.9374e-04j,\n",
       "         -9.1007e-07-1.2207e-03j, -3.2644e-06-2.0302e-03j,\n",
       "         -4.6189e-07-2.8280e-04j, -6.9343e-07-2.2963e-04j,\n",
       "         -6.0116e-07-3.6674e-04j, -1.8925e-06-6.9532e-04j],\n",
       "        [-4.4994e-06-2.1692e-03j, -1.4063e-08-1.8166e-04j,\n",
       "         -8.0005e-07-1.1446e-03j, -2.8699e-06-1.9036e-03j,\n",
       "         -4.0606e-07-2.6516e-04j, -6.0963e-07-2.1531e-04j,\n",
       "         -5.2850e-07-3.4387e-04j, -1.6638e-06-6.5196e-04j],\n",
       "        [-3.9474e-06-2.0318e-03j, -1.2338e-08-1.7015e-04j,\n",
       "         -7.0190e-07-1.0721e-03j, -2.5178e-06-1.7830e-03j,\n",
       "         -3.5625e-07-2.4836e-04j, -5.3484e-07-2.0167e-04j,\n",
       "         -4.6366e-07-3.2209e-04j, -1.4597e-06-6.1066e-04j],\n",
       "        [-3.4537e-06-1.9005e-03j, -1.0790e-08-1.5916e-04j,\n",
       "         -6.1410e-07-1.0028e-03j, -2.2029e-06-1.6678e-03j,\n",
       "         -3.1169e-07-2.3232e-04j, -4.6796e-07-1.8864e-04j,\n",
       "         -4.0568e-07-3.0128e-04j, -1.2771e-06-5.7121e-04j],\n",
       "        [-3.0117e-06-1.7747e-03j, -9.4080e-09-1.4862e-04j,\n",
       "         -5.3549e-07-9.3643e-04j, -1.9209e-06-1.5574e-03j,\n",
       "         -2.7180e-07-2.1694e-04j, -4.0806e-07-1.7615e-04j,\n",
       "         -3.5375e-07-2.8134e-04j, -1.1137e-06-5.3340e-04j],\n",
       "        [-2.6155e-06-1.6539e-03j, -8.1748e-09-1.3850e-04j,\n",
       "         -4.6506e-07-8.7266e-04j, -1.6682e-06-1.4513e-03j,\n",
       "         -2.3604e-07-2.0217e-04j, -3.5438e-07-1.6416e-04j,\n",
       "         -3.0722e-07-2.6218e-04j, -9.6715e-07-4.9707e-04j],\n",
       "        [-2.2601e-06-1.5374e-03j, -7.0656e-09-1.2875e-04j,\n",
       "         -4.0188e-07-8.1121e-04j, -1.4416e-06-1.3491e-03j,\n",
       "         -2.0397e-07-1.8793e-04j, -3.0623e-07-1.5260e-04j,\n",
       "         -2.6548e-07-2.4371e-04j, -8.3575e-07-4.6207e-04j],\n",
       "        [-1.9414e-06-1.4249e-03j, -6.0692e-09-1.1933e-04j,\n",
       "         -3.4522e-07-7.5185e-04j, -1.2383e-06-1.2504e-03j,\n",
       "         -1.7521e-07-1.7418e-04j, -2.6305e-07-1.4143e-04j,\n",
       "         -2.2804e-07-2.2588e-04j, -7.1791e-07-4.2826e-04j],\n",
       "        [-1.6559e-06-1.3160e-03j, -5.1761e-09-1.1020e-04j,\n",
       "         -2.9444e-07-6.9436e-04j, -1.0562e-06-1.1548e-03j,\n",
       "         -1.4944e-07-1.6086e-04j, -2.2436e-07-1.3062e-04j,\n",
       "         -1.9450e-07-2.0861e-04j, -6.1231e-07-3.9551e-04j],\n",
       "        [-1.4003e-06-1.2102e-03j, -4.3761e-09-1.0134e-04j,\n",
       "         -2.4900e-07-6.3854e-04j, -8.9320e-07-1.0620e-03j,\n",
       "         -1.2638e-07-1.4793e-04j, -1.8974e-07-1.2012e-04j,\n",
       "         -1.6449e-07-1.9184e-04j, -5.1783e-07-3.6372e-04j],\n",
       "        [-1.1722e-06-1.1072e-03j, -3.6589e-09-9.2722e-05j,\n",
       "         -2.0841e-07-5.8422e-04j, -7.4766e-07-9.7164e-04j,\n",
       "         -1.0579e-07-1.3535e-04j, -1.5883e-07-1.0990e-04j,\n",
       "         -1.3768e-07-1.7552e-04j, -4.3346e-07-3.3278e-04j],\n",
       "        [-9.6929e-07-1.0068e-03j, -3.0307e-09-8.4314e-05j,\n",
       "         -1.7236e-07-5.3124e-04j, -6.1826e-07-8.8352e-04j,\n",
       "         -8.7477e-08-1.2307e-04j, -1.3133e-07-9.9932e-05j,\n",
       "         -1.1385e-07-1.5960e-04j, -3.5842e-07-3.0260e-04j],\n",
       "        [-7.8951e-07-9.0865e-04j, -2.4702e-09-7.6093e-05j,\n",
       "         -1.4040e-07-4.7945e-04j, -5.0359e-07-7.9738e-04j,\n",
       "         -7.1253e-08-1.1107e-04j, -1.0697e-07-9.0189e-05j,\n",
       "         -9.2738e-08-1.4404e-04j, -2.9194e-07-2.7310e-04j],\n",
       "        [-6.3123e-07-8.1247e-04j, -1.9762e-09-6.8038e-05j,\n",
       "         -1.1226e-07-4.2870e-04j, -4.0264e-07-7.1297e-04j,\n",
       "         -5.6969e-08-9.9315e-05j, -8.5526e-08-8.0642e-05j,\n",
       "         -7.4147e-08-1.2880e-04j, -2.3342e-07-2.4419e-04j],\n",
       "        [-4.9293e-07-7.1803e-04j, -1.5365e-09-6.0130e-05j,\n",
       "         -8.7631e-08-3.7887e-04j, -3.1440e-07-6.3010e-04j,\n",
       "         -4.4485e-08-8.7771e-05j, -6.6791e-08-7.1269e-05j,\n",
       "         -5.7898e-08-1.1382e-04j, -1.8228e-07-2.1581e-04j],\n",
       "        [-3.7364e-07-6.2510e-04j, -1.1682e-09-5.2348e-05j,\n",
       "         -6.6439e-08-3.2983e-04j, -2.3832e-07-5.4855e-04j,\n",
       "         -3.3720e-08-7.6411e-05j, -5.0625e-08-6.2045e-05j,\n",
       "         -4.3888e-08-9.9093e-05j, -1.3816e-07-1.8788e-04j],\n",
       "        [-2.7213e-07-5.3347e-04j, -8.5099e-10-4.4674e-05j,\n",
       "         -4.8390e-08-2.8148e-04j, -1.7358e-07-4.6814e-04j,\n",
       "         -2.4559e-08-6.5210e-05j, -3.6871e-08-5.2950e-05j,\n",
       "         -3.1965e-08-8.4567e-05j, -1.0063e-07-1.6034e-04j],\n",
       "        [-1.8761e-07-4.4293e-04j, -5.8828e-10-3.7092e-05j,\n",
       "         -3.3369e-08-2.3371e-04j, -1.1967e-07-3.8869e-04j,\n",
       "         -1.6933e-08-5.4143e-05j, -2.5420e-08-4.3963e-05j,\n",
       "         -2.2038e-08-7.0214e-05j, -6.9375e-08-1.3312e-04j],\n",
       "        [-1.1936e-07-3.5329e-04j, -3.7431e-10-2.9585e-05j,\n",
       "         -2.1229e-08-1.8641e-04j, -7.6136e-08-3.1002e-04j,\n",
       "         -1.0772e-08-4.3185e-05j, -1.6172e-08-3.5066e-05j,\n",
       "         -1.4021e-08-5.6004e-05j, -4.4136e-08-1.0618e-04j],\n",
       "        [-6.6807e-08-2.6435e-04j, -2.0776e-10-2.2137e-05j,\n",
       "         -1.1875e-08-1.3948e-04j, -4.2609e-08-2.3198e-04j,\n",
       "         -6.0288e-09-3.2314e-05j, -9.0524e-09-2.6238e-05j,\n",
       "         -7.8467e-09-4.1906e-05j, -2.4705e-08-7.9451e-05j],\n",
       "        [-2.9624e-08-1.7594e-04j, -9.4539e-11-1.4734e-05j,\n",
       "         -5.2760e-09-9.2836e-05j, -1.8901e-08-1.5440e-04j,\n",
       "         -2.6743e-09-2.1507e-05j, -4.0130e-09-1.7463e-05j,\n",
       "         -3.4807e-09-2.7891e-05j, -1.0953e-08-5.2880e-05j],\n",
       "        [-7.3596e-09-8.7884e-05j, -2.0926e-11-7.3597e-06j,\n",
       "         -1.2996e-09-4.6372e-05j, -4.6881e-09-7.7122e-05j,\n",
       "         -6.6337e-10-1.0743e-05j, -9.9811e-10-8.7231e-06j,\n",
       "         -8.6341e-10-1.3932e-05j, -2.7232e-09-2.6414e-05j],\n",
       "        [-5.3576e-18+1.2224e-10j, -4.4751e-19+1.0237e-11j,\n",
       "         -2.8219e-18+6.4500e-11j, -4.6981e-18+1.0727e-10j,\n",
       "         -6.5445e-19+1.4942e-11j, -5.3229e-19+1.2133e-11j,\n",
       "         -8.4872e-19+1.9378e-11j, -1.6112e-18+3.6740e-11j],\n",
       "        [-7.4167e-09+8.7885e-05j, -2.5694e-11+7.3598e-06j,\n",
       "         -1.3297e-09+4.6372e-05j, -4.7381e-09+7.7123e-05j,\n",
       "         -6.7034e-10+1.0743e-05j, -1.0038e-09+8.7232e-06j,\n",
       "         -8.7245e-10+1.3932e-05j, -2.7404e-09+2.6414e-05j],\n",
       "        [-2.9571e-08+1.7594e-04j, -9.0116e-11+1.4734e-05j,\n",
       "         -5.2482e-09+9.2836e-05j, -1.8855e-08+1.5440e-04j,\n",
       "         -2.6679e-09+2.1507e-05j, -4.0078e-09+1.7463e-05j,\n",
       "         -3.4723e-09+2.7891e-05j, -1.0937e-08+5.2880e-05j],\n",
       "        [-6.6852e-08+2.6435e-04j, -2.1153e-10+2.2137e-05j,\n",
       "         -1.1898e-08+1.3948e-04j, -4.2649e-08+2.3198e-04j,\n",
       "         -6.0343e-09+3.2314e-05j, -9.0569e-09+2.6238e-05j,\n",
       "         -7.8539e-09+4.1906e-05j, -2.4719e-08+7.9451e-05j],\n",
       "        [-1.1934e-07+3.5329e-04j, -3.7240e-10+2.9585e-05j,\n",
       "         -2.1217e-08+1.8641e-04j, -7.6116e-08+3.1002e-04j,\n",
       "         -1.0770e-08+4.3185e-05j, -1.6169e-08+3.5066e-05j,\n",
       "         -1.4017e-08+5.6004e-05j, -4.4129e-08+1.0618e-04j],\n",
       "        [-1.8762e-07+4.4293e-04j, -5.8871e-10+3.7092e-05j,\n",
       "         -3.3371e-08+2.3371e-04j, -1.1968e-07+3.8869e-04j,\n",
       "         -1.6933e-08+5.4143e-05j, -2.5420e-08+4.3963e-05j,\n",
       "         -2.2039e-08+7.0214e-05j, -6.9377e-08+1.3312e-04j],\n",
       "        [-2.7209e-07+5.3347e-04j, -8.4795e-10+4.4674e-05j,\n",
       "         -4.8371e-08+2.8148e-04j, -1.7354e-07+4.6814e-04j,\n",
       "         -2.4555e-08+6.5210e-05j, -3.6868e-08+5.2950e-05j,\n",
       "         -3.1959e-08+8.4567e-05j, -1.0062e-07+1.6034e-04j],\n",
       "        [-3.7365e-07+6.2510e-04j, -1.1691e-09+5.2348e-05j,\n",
       "         -6.6445e-08+3.2983e-04j, -2.3833e-07+5.4855e-04j,\n",
       "         -3.3722e-08+7.6411e-05j, -5.0626e-08+6.2045e-05j,\n",
       "         -4.3890e-08+9.9093e-05j, -1.3817e-07+1.8788e-04j],\n",
       "        [-4.9297e-07+7.1803e-04j, -1.5402e-09+6.0130e-05j,\n",
       "         -8.7655e-08+3.7887e-04j, -3.1444e-07+6.3010e-04j,\n",
       "         -4.4490e-08+8.7771e-05j, -6.6795e-08+7.1269e-05j,\n",
       "         -5.7905e-08+1.1382e-04j, -1.8229e-07+2.1581e-04j],\n",
       "        [-6.3117e-07+8.1247e-04j, -1.9709e-09+6.8038e-05j,\n",
       "         -1.1222e-07+4.2870e-04j, -4.0258e-07+7.1297e-04j,\n",
       "         -5.6961e-08+9.9315e-05j, -8.5520e-08+8.0643e-05j,\n",
       "         -7.4137e-08+1.2880e-04j, -2.3340e-07+2.4419e-04j],\n",
       "        [-7.8951e-07+9.0865e-04j, -2.4703e-09+7.6093e-05j,\n",
       "         -1.4040e-07+4.7945e-04j, -5.0359e-07+7.9738e-04j,\n",
       "         -7.1253e-08+1.1107e-04j, -1.0697e-07+9.0189e-05j,\n",
       "         -9.2738e-08+1.4404e-04j, -2.9195e-07+2.7310e-04j],\n",
       "        [-9.6932e-07+1.0068e-03j, -3.0334e-09+8.4314e-05j,\n",
       "         -1.7237e-07+5.3124e-04j, -6.1828e-07+8.8352e-04j,\n",
       "         -8.7481e-08+1.2307e-04j, -1.3133e-07+9.9933e-05j,\n",
       "         -1.1386e-07+1.5960e-04j, -3.5843e-07+3.0260e-04j],\n",
       "        [-1.1723e-06+1.1072e-03j, -3.6646e-09+9.2722e-05j,\n",
       "         -2.0845e-07+5.8422e-04j, -7.4772e-07+9.7164e-04j,\n",
       "         -1.0580e-07+1.3535e-04j, -1.5883e-07+1.0990e-04j,\n",
       "         -1.3770e-07+1.7552e-04j, -4.3348e-07+3.3278e-04j],\n",
       "        [-1.4003e-06+1.2102e-03j, -4.3746e-09+1.0134e-04j,\n",
       "         -2.4899e-07+6.3854e-04j, -8.9318e-07+1.0620e-03j,\n",
       "         -1.2638e-07+1.4793e-04j, -1.8974e-07+1.2012e-04j,\n",
       "         -1.6448e-07+1.9184e-04j, -5.1782e-07+3.6372e-04j],\n",
       "        [-1.6558e-06+1.3160e-03j, -5.1727e-09+1.1020e-04j,\n",
       "         -2.9442e-07+6.9436e-04j, -1.0562e-06+1.1548e-03j,\n",
       "         -1.4944e-07+1.6086e-04j, -2.2436e-07+1.3062e-04j,\n",
       "         -1.9450e-07+2.0861e-04j, -6.1230e-07+3.9551e-04j],\n",
       "        [-1.9413e-06+1.4249e-03j, -6.0600e-09+1.1933e-04j,\n",
       "         -3.4516e-07+7.5185e-04j, -1.2382e-06+1.2504e-03j,\n",
       "         -1.7520e-07+1.7418e-04j, -2.6304e-07+1.4143e-04j,\n",
       "         -2.2803e-07+2.2588e-04j, -7.1787e-07+4.2826e-04j],\n",
       "        [-2.2600e-06+1.5374e-03j, -7.0603e-09+1.2875e-04j,\n",
       "         -4.0185e-07+8.1121e-04j, -1.4415e-06+1.3491e-03j,\n",
       "         -2.0396e-07+1.8793e-04j, -3.0622e-07+1.5260e-04j,\n",
       "         -2.6547e-07+2.4372e-04j, -8.3573e-07+4.6207e-04j],\n",
       "        [-2.6155e-06+1.6539e-03j, -8.1748e-09+1.3850e-04j,\n",
       "         -4.6506e-07+8.7266e-04j, -1.6682e-06+1.4513e-03j,\n",
       "         -2.3604e-07+2.0217e-04j, -3.5438e-07+1.6416e-04j,\n",
       "         -3.0722e-07+2.6218e-04j, -9.6715e-07+4.9707e-04j],\n",
       "        [-3.0117e-06+1.7747e-03j, -9.4129e-09+1.4862e-04j,\n",
       "         -5.3552e-07+9.3643e-04j, -1.9210e-06+1.5574e-03j,\n",
       "         -2.7180e-07+2.1694e-04j, -4.0807e-07+1.7615e-04j,\n",
       "         -3.5376e-07+2.8134e-04j, -1.1137e-06+5.3340e-04j],\n",
       "        [-3.4538e-06+1.9005e-03j, -1.0797e-08+1.5916e-04j,\n",
       "         -6.1414e-07+1.0028e-03j, -2.2030e-06+1.6678e-03j,\n",
       "         -3.1170e-07+2.3232e-04j, -4.6796e-07+1.8864e-04j,\n",
       "         -4.0569e-07+3.0128e-04j, -1.2771e-06+5.7121e-04j],\n",
       "        [-3.9473e-06+2.0318e-03j, -1.2330e-08+1.7015e-04j,\n",
       "         -7.0185e-07+1.0721e-03j, -2.5177e-06+1.7830e-03j,\n",
       "         -3.5624e-07+2.4836e-04j, -5.3483e-07+2.0167e-04j,\n",
       "         -4.6365e-07+3.2209e-04j, -1.4596e-06+6.1066e-04j],\n",
       "        [-4.4994e-06+2.1692e-03j, -1.4068e-08+1.8166e-04j,\n",
       "         -8.0008e-07+1.1446e-03j, -2.8699e-06+1.9036e-03j,\n",
       "         -4.0607e-07+2.6516e-04j, -6.0964e-07+2.1531e-04j,\n",
       "         -5.2851e-07+3.4387e-04j, -1.6638e-06+6.5196e-04j],\n",
       "        [-5.1178e-06+2.3135e-03j, -1.5997e-08+1.9374e-04j,\n",
       "         -9.1002e-07+1.2207e-03j, -3.2644e-06+2.0302e-03j,\n",
       "         -4.6188e-07+2.8280e-04j, -6.9343e-07+2.2963e-04j,\n",
       "         -6.0115e-07+3.6674e-04j, -1.8925e-06+6.9532e-04j],\n",
       "        [-5.8124e-06+2.4655e-03j, -1.8174e-08+2.0647e-04j,\n",
       "         -1.0336e-06+1.3009e-03j, -3.7074e-06+2.1636e-03j,\n",
       "         -5.2456e-07+3.0138e-04j, -7.8753e-07+2.4471e-04j,\n",
       "         -6.8274e-07+3.9083e-04j, -2.1493e-06+7.4100e-04j],\n",
       "        [-6.5944e-06+2.6261e-03j, -2.0610e-08+2.1992e-04j,\n",
       "         -1.1726e-06+1.3857e-03j, -4.2062e-06+2.3045e-03j,\n",
       "         -5.9514e-07+3.2101e-04j, -8.9349e-07+2.6066e-04j,\n",
       "         -7.7459e-07+4.1630e-04j, -2.4385e-06+7.8928e-04j],\n",
       "        [-7.4780e-06+2.7965e-03j, -2.3375e-08+2.3419e-04j,\n",
       "         -1.3297e-06+1.4756e-03j, -4.7698e-06+2.4541e-03j,\n",
       "         -6.7488e-07+3.4184e-04j, -1.0132e-06+2.7757e-04j,\n",
       "         -8.7838e-07+4.4331e-04j, -2.7652e-06+8.4050e-04j],\n",
       "        [-8.4802e-06+2.9780e-03j, -2.6518e-08+2.4939e-04j,\n",
       "         -1.5080e-06+1.5713e-03j, -5.4091e-06+2.6133e-03j,\n",
       "         -7.6533e-07+3.6403e-04j, -1.1490e-06+2.9558e-04j,\n",
       "         -9.9610e-07+4.7208e-04j, -3.1358e-06+8.9504e-04j],\n",
       "        [-9.6212e-06+3.1720e-03j, -3.0084e-08+2.6564e-04j,\n",
       "         -1.7108e-06+1.6737e-03j, -6.1369e-06+2.7836e-03j,\n",
       "         -8.6831e-07+3.8774e-04j, -1.3036e-06+3.1484e-04j,\n",
       "         -1.1301e-06+5.0284e-04j, -3.5577e-06+9.5336e-04j],\n",
       "        [-1.0927e-05+3.3804e-03j, -3.4153e-08+2.8309e-04j,\n",
       "         -1.9429e-06+1.7837e-03j, -6.9695e-06+2.9665e-03j,\n",
       "         -9.8612e-07+4.1322e-04j, -1.4805e-06+3.3552e-04j,\n",
       "         -1.2835e-06+5.3587e-04j, -4.0405e-06+1.0160e-03j],\n",
       "        [-1.2429e-05+3.6052e-03j, -3.8846e-08+3.0192e-04j,\n",
       "         -2.2100e-06+1.9023e-03j, -7.9275e-06+3.1638e-03j,\n",
       "         -1.1217e-06+4.4070e-04j, -1.6840e-06+3.5784e-04j,\n",
       "         -1.4599e-06+5.7152e-04j, -4.5958e-06+1.0836e-03j],\n",
       "        [-1.4166e-05+3.8491e-03j, -4.4281e-08+3.2233e-04j,\n",
       "         -2.5190e-06+2.0310e-03j, -9.0360e-06+3.3777e-03j,\n",
       "         -1.2785e-06+4.7050e-04j, -1.9194e-06+3.8204e-04j,\n",
       "         -1.6640e-06+6.1017e-04j, -5.2385e-06+1.1568e-03j],\n",
       "        [-1.6191e-05+4.1149e-03j, -5.0614e-08+3.4460e-04j,\n",
       "         -2.8791e-06+2.1713e-03j, -1.0328e-05+3.6110e-03j,\n",
       "         -1.4612e-06+5.0300e-04j, -2.1938e-06+4.0843e-04j,\n",
       "         -1.9019e-06+6.5231e-04j, -5.9872e-06+1.2367e-03j],\n",
       "        [-1.8568e-05+4.4066e-03j, -5.8048e-08+3.6902e-04j,\n",
       "         -3.3017e-06+2.3251e-03j, -1.1843e-05+3.8670e-03j,\n",
       "         -1.6757e-06+5.3866e-04j, -2.5158e-06+4.3737e-04j,\n",
       "         -2.1810e-06+6.9855e-04j, -6.8659e-06+1.3244e-03j],\n",
       "        [-2.1381e-05+4.7286e-03j, -6.6835e-08+3.9599e-04j,\n",
       "         -3.8019e-06+2.4951e-03j, -1.3638e-05+4.1496e-03j,\n",
       "         -1.9296e-06+5.7802e-04j, -2.8969e-06+4.6933e-04j,\n",
       "         -2.5114e-06+7.4960e-04j, -7.9061e-06+1.4212e-03j],\n",
       "        [-2.4742e-05+5.0868e-03j, -7.7340e-08+4.2599e-04j,\n",
       "         -4.3996e-06+2.6841e-03j, -1.5782e-05+4.4639e-03j,\n",
       "         -2.2330e-06+6.2180e-04j, -3.3524e-06+5.0488e-04j,\n",
       "         -2.9063e-06+8.0638e-04j, -9.1492e-06+1.5288e-03j],\n",
       "        [-2.8804e-05+5.4884e-03j, -9.0036e-08+4.5962e-04j,\n",
       "         -5.1218e-06+2.8960e-03j, -1.8372e-05+4.8163e-03j,\n",
       "         -2.5995e-06+6.7090e-04j, -3.9026e-06+5.4474e-04j,\n",
       "         -3.3833e-06+8.7004e-04j, -1.0651e-05+1.6495e-03j],\n",
       "        [-3.3771e-05+5.9428e-03j, -1.0557e-07+4.9768e-04j,\n",
       "         -6.0051e-06+3.1358e-03j, -2.1541e-05+5.2151e-03j,\n",
       "         -3.0478e-06+7.2644e-04j, -4.5756e-06+5.8984e-04j,\n",
       "         -3.9668e-06+9.4208e-04j, -1.2488e-05+1.7861e-03j],\n",
       "        [-3.9932e-05+6.4622e-03j, -1.2482e-07+5.4118e-04j,\n",
       "         -7.1007e-06+3.4099e-03j, -2.5471e-05+5.6709e-03j,\n",
       "         -3.6039e-06+7.8994e-04j, -5.4104e-06+6.4139e-04j,\n",
       "         -4.6906e-06+1.0244e-03j, -1.4766e-05+1.9422e-03j],\n",
       "        [-4.7702e-05+7.0629e-03j, -1.4909e-07+5.9150e-04j,\n",
       "         -8.4823e-06+3.7269e-03j, -3.0427e-05+6.1981e-03j,\n",
       "         -4.3051e-06+8.6338e-04j, -6.4630e-06+7.0101e-04j,\n",
       "         -5.6032e-06+1.1197e-03j, -1.7639e-05+2.1227e-03j],\n",
       "        [-5.7691e-05+7.7673e-03j, -1.8032e-07+6.5049e-04j,\n",
       "         -1.0259e-05+4.0986e-03j, -3.6798e-05+6.8163e-03j,\n",
       "         -5.2066e-06+9.4948e-04j, -7.8163e-06+7.7091e-04j,\n",
       "         -6.7766e-06+1.2313e-03j, -2.1332e-05+2.3344e-03j],\n",
       "        [-7.0829e-05+8.6063e-03j, -2.2144e-07+7.2076e-04j,\n",
       "         -1.2595e-05+4.5413e-03j, -4.5179e-05+7.5526e-03j,\n",
       "         -6.3924e-06+1.0521e-03j, -9.5963e-06+8.5418e-04j,\n",
       "         -8.3199e-06+1.3643e-03j, -2.6190e-05+2.5866e-03j],\n",
       "        [-8.8590e-05+9.6249e-03j, -2.7696e-07+8.0609e-04j,\n",
       "         -1.5754e-05+5.0789e-03j, -5.6509e-05+8.4466e-03j,\n",
       "         -7.9954e-06+1.1766e-03j, -1.2002e-05+9.5526e-04j,\n",
       "         -1.0406e-05+1.5258e-03j, -3.2757e-05+2.8927e-03j],\n",
       "        [-1.1342e-04+1.0891e-02j, -3.5458e-07+9.1211e-04j,\n",
       "         -2.0170e-05+5.7469e-03j, -7.2350e-05+9.5574e-03j,\n",
       "         -1.0237e-05+1.3313e-03j, -1.5367e-05+1.0809e-03j,\n",
       "         -1.3323e-05+1.7265e-03j, -4.1939e-05+3.2730e-03j],\n",
       "        [-1.4965e-04+1.2509e-02j, -4.6784e-07+1.0477e-03j,\n",
       "         -2.6613e-05+6.6013e-03j, -9.5458e-05+1.0978e-02j,\n",
       "         -1.3506e-05+1.5292e-03j, -2.0274e-05+1.2415e-03j,\n",
       "         -1.7579e-05+1.9831e-03j, -5.5333e-05+3.7594e-03j],\n",
       "        [-2.0546e-04+1.4657e-02j, -6.4236e-07+1.2277e-03j,\n",
       "         -3.6540e-05+7.7351e-03j, -1.3106e-04+1.2863e-02j,\n",
       "         -1.8544e-05+1.7918e-03j, -2.7834e-05+1.4546e-03j,\n",
       "         -2.4136e-05+2.3237e-03j, -7.5968e-05+4.4048e-03j],\n",
       "        [-2.9802e-04+1.7652e-02j, -9.3173e-07+1.4786e-03j,\n",
       "         -5.3005e-05+9.3162e-03j, -1.9011e-04+1.5492e-02j,\n",
       "         -2.6899e-05+2.1580e-03j, -4.0369e-05+1.7516e-03j,\n",
       "         -3.5010e-05+2.7986e-03j, -1.1019e-04+5.3045e-03j],\n",
       "        [-4.6837e-04+2.2127e-02j, -1.4646e-06+1.8538e-03j,\n",
       "         -8.3315e-05+1.1680e-02j, -2.9881e-04+1.9421e-02j,\n",
       "         -4.2278e-05+2.7053e-03j, -6.3436e-05+2.1954e-03j,\n",
       "         -5.5026e-05+3.5083e-03j, -1.7315e-04+6.6488e-03j],\n",
       "        [-8.3623e-04+2.9561e-02j, -2.6162e-06+2.4775e-03j,\n",
       "         -1.4880e-04+1.5609e-02j, -5.3358e-04+2.5950e-02j,\n",
       "         -7.5496e-05+3.6148e-03j, -1.1323e-04+2.9321e-03j,\n",
       "         -9.8260e-05+4.6877e-03j, -3.0910e-04+8.8809e-03j],\n",
       "        [-1.8858e-03+4.4370e-02j, -5.9056e-06+3.7224e-03j,\n",
       "         -3.3587e-04+2.3449e-02j, -1.2039e-03+3.8969e-02j,\n",
       "         -1.7034e-04+5.4281e-03j, -2.5512e-04+4.3971e-03j,\n",
       "         -2.2169e-04+7.0394e-03j, -6.9671e-04+1.3323e-02j],\n",
       "        [-7.5175e-03+8.8348e-02j, -2.3667e-05+7.4520e-03j,\n",
       "         -1.3453e-03+4.6915e-02j, -4.8113e-03+7.7793e-02j,\n",
       "         -6.8067e-04+1.0835e-02j, -1.0123e-03+8.7150e-03j,\n",
       "         -8.8589e-04+1.4051e-02j, -2.7696e-03+2.6456e-02j]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abcedfc0-3cb5-499a-9d72-ffaf7dfb291b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.3481e-01+0.0000e+00j, -1.9963e-02+0.0000e+00j,\n",
       "         -1.8023e-01+0.0000e+00j, -3.8873e-01+0.0000e+00j,\n",
       "         -3.9485e-01+0.0000e+00j, -7.3006e-01+0.0000e+00j,\n",
       "         -3.9628e-01+0.0000e+00j, -6.5800e-01+0.0000e+00j],\n",
       "        [-5.3481e-01+6.2853e+00j, -1.9963e-02+6.2853e+00j,\n",
       "         -1.8023e-01+6.2853e+00j, -3.8873e-01+6.2853e+00j,\n",
       "         -3.9485e-01+6.2853e+00j, -7.3006e-01+6.2853e+00j,\n",
       "         -3.9628e-01+6.2853e+00j, -6.5800e-01+6.2853e+00j],\n",
       "        [-5.3481e-01+1.2583e+01j, -1.9965e-02+1.2583e+01j,\n",
       "         -1.8023e-01+1.2583e+01j, -3.8873e-01+1.2583e+01j,\n",
       "         -3.9485e-01+1.2583e+01j, -7.3006e-01+1.2583e+01j,\n",
       "         -3.9628e-01+1.2583e+01j, -6.5800e-01+1.2583e+01j],\n",
       "        [-5.3481e-01+1.8906e+01j, -1.9960e-02+1.8906e+01j,\n",
       "         -1.8023e-01+1.8906e+01j, -3.8873e-01+1.8906e+01j,\n",
       "         -3.9485e-01+1.8906e+01j, -7.3006e-01+1.8906e+01j,\n",
       "         -3.9628e-01+1.8906e+01j, -6.5800e-01+1.8906e+01j],\n",
       "        [-5.3481e-01+2.5266e+01j, -1.9964e-02+2.5266e+01j,\n",
       "         -1.8023e-01+2.5266e+01j, -3.8873e-01+2.5266e+01j,\n",
       "         -3.9485e-01+2.5266e+01j, -7.3006e-01+2.5266e+01j,\n",
       "         -3.9628e-01+2.5266e+01j, -6.5800e-01+2.5266e+01j],\n",
       "        [-5.3481e-01+3.1677e+01j, -1.9965e-02+3.1677e+01j,\n",
       "         -1.8023e-01+3.1677e+01j, -3.8873e-01+3.1677e+01j,\n",
       "         -3.9485e-01+3.1677e+01j, -7.3006e-01+3.1677e+01j,\n",
       "         -3.9628e-01+3.1677e+01j, -6.5800e-01+3.1677e+01j],\n",
       "        [-5.3481e-01+3.8152e+01j, -1.9963e-02+3.8152e+01j,\n",
       "         -1.8023e-01+3.8152e+01j, -3.8873e-01+3.8152e+01j,\n",
       "         -3.9485e-01+3.8152e+01j, -7.3006e-01+3.8152e+01j,\n",
       "         -3.9628e-01+3.8152e+01j, -6.5800e-01+3.8152e+01j],\n",
       "        [-5.3481e-01+4.4705e+01j, -1.9964e-02+4.4705e+01j,\n",
       "         -1.8023e-01+4.4705e+01j, -3.8873e-01+4.4705e+01j,\n",
       "         -3.9485e-01+4.4705e+01j, -7.3006e-01+4.4705e+01j,\n",
       "         -3.9628e-01+4.4705e+01j, -6.5800e-01+4.4705e+01j],\n",
       "        [-5.3481e-01+5.1351e+01j, -1.9962e-02+5.1351e+01j,\n",
       "         -1.8023e-01+5.1351e+01j, -3.8873e-01+5.1351e+01j,\n",
       "         -3.9485e-01+5.1351e+01j, -7.3006e-01+5.1351e+01j,\n",
       "         -3.9628e-01+5.1351e+01j, -6.5800e-01+5.1351e+01j],\n",
       "        [-5.3481e-01+5.8105e+01j, -1.9963e-02+5.8105e+01j,\n",
       "         -1.8023e-01+5.8105e+01j, -3.8873e-01+5.8105e+01j,\n",
       "         -3.9485e-01+5.8105e+01j, -7.3006e-01+5.8105e+01j,\n",
       "         -3.9628e-01+5.8105e+01j, -6.5800e-01+5.8105e+01j],\n",
       "        [-5.3481e-01+6.4984e+01j, -1.9963e-02+6.4984e+01j,\n",
       "         -1.8023e-01+6.4984e+01j, -3.8873e-01+6.4984e+01j,\n",
       "         -3.9485e-01+6.4984e+01j, -7.3006e-01+6.4984e+01j,\n",
       "         -3.9628e-01+6.4984e+01j, -6.5800e-01+6.4984e+01j],\n",
       "        [-5.3481e-01+7.2004e+01j, -1.9966e-02+7.2004e+01j,\n",
       "         -1.8023e-01+7.2004e+01j, -3.8873e-01+7.2004e+01j,\n",
       "         -3.9485e-01+7.2004e+01j, -7.3007e-01+7.2004e+01j,\n",
       "         -3.9628e-01+7.2004e+01j, -6.5800e-01+7.2004e+01j],\n",
       "        [-5.3481e-01+7.9186e+01j, -1.9964e-02+7.9186e+01j,\n",
       "         -1.8023e-01+7.9186e+01j, -3.8873e-01+7.9186e+01j,\n",
       "         -3.9485e-01+7.9186e+01j, -7.3006e-01+7.9186e+01j,\n",
       "         -3.9628e-01+7.9186e+01j, -6.5800e-01+7.9186e+01j],\n",
       "        [-5.3481e-01+8.6548e+01j, -1.9965e-02+8.6548e+01j,\n",
       "         -1.8023e-01+8.6548e+01j, -3.8873e-01+8.6548e+01j,\n",
       "         -3.9485e-01+8.6548e+01j, -7.3006e-01+8.6548e+01j,\n",
       "         -3.9628e-01+8.6548e+01j, -6.5800e-01+8.6548e+01j],\n",
       "        [-5.3481e-01+9.4113e+01j, -1.9961e-02+9.4113e+01j,\n",
       "         -1.8023e-01+9.4113e+01j, -3.8873e-01+9.4113e+01j,\n",
       "         -3.9485e-01+9.4113e+01j, -7.3006e-01+9.4113e+01j,\n",
       "         -3.9628e-01+9.4113e+01j, -6.5800e-01+9.4113e+01j],\n",
       "        [-5.3481e-01+1.0191e+02j, -1.9964e-02+1.0191e+02j,\n",
       "         -1.8023e-01+1.0191e+02j, -3.8873e-01+1.0191e+02j,\n",
       "         -3.9485e-01+1.0191e+02j, -7.3006e-01+1.0191e+02j,\n",
       "         -3.9628e-01+1.0191e+02j, -6.5800e-01+1.0191e+02j],\n",
       "        [-5.3481e-01+1.0995e+02j, -1.9960e-02+1.0995e+02j,\n",
       "         -1.8023e-01+1.0995e+02j, -3.8872e-01+1.0995e+02j,\n",
       "         -3.9485e-01+1.0995e+02j, -7.3006e-01+1.0995e+02j,\n",
       "         -3.9628e-01+1.0995e+02j, -6.5800e-01+1.0995e+02j],\n",
       "        [-5.3481e-01+1.1828e+02j, -1.9962e-02+1.1828e+02j,\n",
       "         -1.8023e-01+1.1828e+02j, -3.8873e-01+1.1828e+02j,\n",
       "         -3.9485e-01+1.1828e+02j, -7.3006e-01+1.1828e+02j,\n",
       "         -3.9628e-01+1.1828e+02j, -6.5800e-01+1.1828e+02j],\n",
       "        [-5.3481e-01+1.2692e+02j, -1.9968e-02+1.2692e+02j,\n",
       "         -1.8023e-01+1.2692e+02j, -3.8873e-01+1.2692e+02j,\n",
       "         -3.9486e-01+1.2692e+02j, -7.3007e-01+1.2692e+02j,\n",
       "         -3.9628e-01+1.2692e+02j, -6.5800e-01+1.2692e+02j],\n",
       "        [-5.3481e-01+1.3592e+02j, -1.9966e-02+1.3592e+02j,\n",
       "         -1.8023e-01+1.3592e+02j, -3.8873e-01+1.3592e+02j,\n",
       "         -3.9485e-01+1.3592e+02j, -7.3007e-01+1.3592e+02j,\n",
       "         -3.9628e-01+1.3592e+02j, -6.5800e-01+1.3592e+02j],\n",
       "        [-5.3481e-01+1.4531e+02j, -1.9965e-02+1.4531e+02j,\n",
       "         -1.8023e-01+1.4531e+02j, -3.8873e-01+1.4531e+02j,\n",
       "         -3.9485e-01+1.4531e+02j, -7.3006e-01+1.4531e+02j,\n",
       "         -3.9628e-01+1.4531e+02j, -6.5800e-01+1.4531e+02j],\n",
       "        [-5.3481e-01+1.5514e+02j, -1.9959e-02+1.5514e+02j,\n",
       "         -1.8023e-01+1.5514e+02j, -3.8872e-01+1.5514e+02j,\n",
       "         -3.9485e-01+1.5514e+02j, -7.3006e-01+1.5514e+02j,\n",
       "         -3.9628e-01+1.5514e+02j, -6.5799e-01+1.5514e+02j],\n",
       "        [-5.3481e-01+1.6545e+02j, -1.9960e-02+1.6545e+02j,\n",
       "         -1.8023e-01+1.6545e+02j, -3.8873e-01+1.6545e+02j,\n",
       "         -3.9485e-01+1.6545e+02j, -7.3006e-01+1.6545e+02j,\n",
       "         -3.9628e-01+1.6545e+02j, -6.5800e-01+1.6545e+02j],\n",
       "        [-5.3482e-01+1.7632e+02j, -1.9969e-02+1.7632e+02j,\n",
       "         -1.8023e-01+1.7632e+02j, -3.8873e-01+1.7632e+02j,\n",
       "         -3.9486e-01+1.7632e+02j, -7.3007e-01+1.7632e+02j,\n",
       "         -3.9629e-01+1.7632e+02j, -6.5800e-01+1.7632e+02j],\n",
       "        [-5.3481e-01+1.8781e+02j, -1.9965e-02+1.8781e+02j,\n",
       "         -1.8023e-01+1.8781e+02j, -3.8873e-01+1.8781e+02j,\n",
       "         -3.9485e-01+1.8781e+02j, -7.3006e-01+1.8781e+02j,\n",
       "         -3.9628e-01+1.8781e+02j, -6.5800e-01+1.8781e+02j],\n",
       "        [-5.3482e-01+2.0000e+02j, -1.9969e-02+2.0000e+02j,\n",
       "         -1.8023e-01+2.0000e+02j, -3.8873e-01+2.0000e+02j,\n",
       "         -3.9486e-01+2.0000e+02j, -7.3007e-01+2.0000e+02j,\n",
       "         -3.9629e-01+2.0000e+02j, -6.5800e-01+2.0000e+02j],\n",
       "        [-5.3480e-01+2.1298e+02j, -1.9955e-02+2.1298e+02j,\n",
       "         -1.8022e-01+2.1298e+02j, -3.8872e-01+2.1298e+02j,\n",
       "         -3.9484e-01+2.1298e+02j, -7.3005e-01+2.1298e+02j,\n",
       "         -3.9627e-01+2.1298e+02j, -6.5799e-01+2.1298e+02j],\n",
       "        [-5.3480e-01+2.2686e+02j, -1.9955e-02+2.2686e+02j,\n",
       "         -1.8022e-01+2.2686e+02j, -3.8872e-01+2.2686e+02j,\n",
       "         -3.9484e-01+2.2686e+02j, -7.3005e-01+2.2686e+02j,\n",
       "         -3.9627e-01+2.2686e+02j, -6.5799e-01+2.2686e+02j],\n",
       "        [-5.3482e-01+2.4176e+02j, -1.9971e-02+2.4176e+02j,\n",
       "         -1.8024e-01+2.4176e+02j, -3.8874e-01+2.4176e+02j,\n",
       "         -3.9486e-01+2.4176e+02j, -7.3007e-01+2.4176e+02j,\n",
       "         -3.9629e-01+2.4176e+02j, -6.5801e-01+2.4176e+02j],\n",
       "        [-5.3481e-01+2.5784e+02j, -1.9960e-02+2.5784e+02j,\n",
       "         -1.8023e-01+2.5784e+02j, -3.8873e-01+2.5784e+02j,\n",
       "         -3.9485e-01+2.5784e+02j, -7.3006e-01+2.5784e+02j,\n",
       "         -3.9628e-01+2.5784e+02j, -6.5800e-01+2.5784e+02j],\n",
       "        [-5.3481e-01+2.7528e+02j, -1.9961e-02+2.7528e+02j,\n",
       "         -1.8023e-01+2.7528e+02j, -3.8873e-01+2.7528e+02j,\n",
       "         -3.9485e-01+2.7528e+02j, -7.3006e-01+2.7528e+02j,\n",
       "         -3.9628e-01+2.7528e+02j, -6.5800e-01+2.7528e+02j],\n",
       "        [-5.3480e-01+2.9429e+02j, -1.9952e-02+2.9429e+02j,\n",
       "         -1.8022e-01+2.9429e+02j, -3.8872e-01+2.9429e+02j,\n",
       "         -3.9484e-01+2.9429e+02j, -7.3005e-01+2.9429e+02j,\n",
       "         -3.9627e-01+2.9429e+02j, -6.5799e-01+2.9429e+02j],\n",
       "        [-5.3480e-01+3.1515e+02j, -1.9949e-02+3.1515e+02j,\n",
       "         -1.8022e-01+3.1515e+02j, -3.8871e-01+3.1515e+02j,\n",
       "         -3.9484e-01+3.1515e+02j, -7.3005e-01+3.1515e+02j,\n",
       "         -3.9627e-01+3.1515e+02j, -6.5799e-01+3.1515e+02j],\n",
       "        [-5.3481e-01+3.3818e+02j, -1.9961e-02+3.3818e+02j,\n",
       "         -1.8023e-01+3.3818e+02j, -3.8873e-01+3.3818e+02j,\n",
       "         -3.9485e-01+3.3818e+02j, -7.3006e-01+3.3818e+02j,\n",
       "         -3.9628e-01+3.3818e+02j, -6.5800e-01+3.3818e+02j],\n",
       "        [-5.3481e-01+3.6380e+02j, -1.9965e-02+3.6380e+02j,\n",
       "         -1.8023e-01+3.6380e+02j, -3.8873e-01+3.6380e+02j,\n",
       "         -3.9485e-01+3.6380e+02j, -7.3006e-01+3.6380e+02j,\n",
       "         -3.9628e-01+3.6380e+02j, -6.5800e-01+3.6380e+02j],\n",
       "        [-5.3481e-01+3.9252e+02j, -1.9965e-02+3.9252e+02j,\n",
       "         -1.8023e-01+3.9252e+02j, -3.8873e-01+3.9252e+02j,\n",
       "         -3.9485e-01+3.9252e+02j, -7.3006e-01+3.9252e+02j,\n",
       "         -3.9628e-01+3.9252e+02j, -6.5800e-01+3.9252e+02j],\n",
       "        [-5.3481e-01+4.2502e+02j, -1.9963e-02+4.2502e+02j,\n",
       "         -1.8023e-01+4.2502e+02j, -3.8873e-01+4.2502e+02j,\n",
       "         -3.9485e-01+4.2502e+02j, -7.3006e-01+4.2502e+02j,\n",
       "         -3.9628e-01+4.2502e+02j, -6.5800e-01+4.2502e+02j],\n",
       "        [-5.3480e-01+4.6217e+02j, -1.9957e-02+4.6217e+02j,\n",
       "         -1.8022e-01+4.6217e+02j, -3.8872e-01+4.6217e+02j,\n",
       "         -3.9485e-01+4.6217e+02j, -7.3006e-01+4.6217e+02j,\n",
       "         -3.9627e-01+4.6217e+02j, -6.5799e-01+4.6217e+02j],\n",
       "        [-5.3478e-01+5.0514e+02j, -1.9933e-02+5.0514e+02j,\n",
       "         -1.8020e-01+5.0514e+02j, -3.8870e-01+5.0514e+02j,\n",
       "         -3.9482e-01+5.0514e+02j, -7.3003e-01+5.0514e+02j,\n",
       "         -3.9625e-01+5.0514e+02j, -6.5797e-01+5.0514e+02j],\n",
       "        [-5.3482e-01+5.5552e+02j, -1.9969e-02+5.5552e+02j,\n",
       "         -1.8023e-01+5.5552e+02j, -3.8873e-01+5.5552e+02j,\n",
       "         -3.9486e-01+5.5552e+02j, -7.3007e-01+5.5552e+02j,\n",
       "         -3.9629e-01+5.5552e+02j, -6.5800e-01+5.5552e+02j],\n",
       "        [-5.3483e-01+6.1554e+02j, -1.9982e-02+6.1554e+02j,\n",
       "         -1.8025e-01+6.1554e+02j, -3.8875e-01+6.1554e+02j,\n",
       "         -3.9487e-01+6.1554e+02j, -7.3008e-01+6.1554e+02j,\n",
       "         -3.9630e-01+6.1554e+02j, -6.5802e-01+6.1554e+02j],\n",
       "        [-5.3484e-01+6.8840e+02j, -1.9995e-02+6.8840e+02j,\n",
       "         -1.8026e-01+6.8840e+02j, -3.8876e-01+6.8840e+02j,\n",
       "         -3.9488e-01+6.8840e+02j, -7.3009e-01+6.8840e+02j,\n",
       "         -3.9631e-01+6.8840e+02j, -6.5803e-01+6.8840e+02j],\n",
       "        [-5.3475e-01+7.7895e+02j, -1.9905e-02+7.7895e+02j,\n",
       "         -1.8017e-01+7.7895e+02j, -3.8867e-01+7.7895e+02j,\n",
       "         -3.9479e-01+7.7895e+02j, -7.3000e-01+7.7895e+02j,\n",
       "         -3.9622e-01+7.7895e+02j, -6.5794e-01+7.7895e+02j],\n",
       "        [-5.3481e-01+8.9475e+02j, -1.9967e-02+8.9475e+02j,\n",
       "         -1.8023e-01+8.9475e+02j, -3.8873e-01+8.9475e+02j,\n",
       "         -3.9485e-01+8.9475e+02j, -7.3007e-01+8.9475e+02j,\n",
       "         -3.9628e-01+8.9475e+02j, -6.5800e-01+8.9475e+02j],\n",
       "        [-5.3482e-01+1.0484e+03j, -1.9971e-02+1.0484e+03j,\n",
       "         -1.8024e-01+1.0484e+03j, -3.8874e-01+1.0484e+03j,\n",
       "         -3.9486e-01+1.0484e+03j, -7.3007e-01+1.0484e+03j,\n",
       "         -3.9629e-01+1.0484e+03j, -6.5801e-01+1.0484e+03j],\n",
       "        [-5.3487e-01+1.2628e+03j, -2.0027e-02+1.2628e+03j,\n",
       "         -1.8029e-01+1.2628e+03j, -3.8879e-01+1.2628e+03j,\n",
       "         -3.9492e-01+1.2628e+03j, -7.3013e-01+1.2628e+03j,\n",
       "         -3.9634e-01+1.2628e+03j, -6.5806e-01+1.2628e+03j],\n",
       "        [-5.3488e-01+1.5832e+03j, -2.0030e-02+1.5832e+03j,\n",
       "         -1.8030e-01+1.5832e+03j, -3.8880e-01+1.5832e+03j,\n",
       "         -3.9492e-01+1.5832e+03j, -7.3013e-01+1.5832e+03j,\n",
       "         -3.9635e-01+1.5832e+03j, -6.5807e-01+1.5832e+03j],\n",
       "        [-5.3470e-01+2.1158e+03j, -1.9856e-02+2.1158e+03j,\n",
       "         -1.8012e-01+2.1158e+03j, -3.8862e-01+2.1158e+03j,\n",
       "         -3.9474e-01+2.1158e+03j, -7.2996e-01+2.1158e+03j,\n",
       "         -3.9617e-01+2.1158e+03j, -6.5789e-01+2.1158e+03j],\n",
       "        [-5.3524e-01+3.1789e+03j, -2.0397e-02+3.1789e+03j,\n",
       "         -1.8066e-01+3.1789e+03j, -3.8916e-01+3.1789e+03j,\n",
       "         -3.9529e-01+3.1789e+03j, -7.3050e-01+3.1789e+03j,\n",
       "         -3.9671e-01+3.1789e+03j, -6.5843e-01+3.1789e+03j],\n",
       "        [-5.3294e-01+6.3641e+03j, -1.8095e-02+6.3641e+03j,\n",
       "         -1.7836e-01+6.3641e+03j, -3.8686e-01+6.3641e+03j,\n",
       "         -3.9298e-01+6.3641e+03j, -7.2819e-01+6.3641e+03j,\n",
       "         -3.9441e-01+6.3641e+03j, -6.5613e-01+6.3641e+03j],\n",
       "        [-2.0053e+02-4.5755e+09j, -2.0002e+02-4.5755e+09j,\n",
       "         -2.0018e+02-4.5755e+09j, -2.0039e+02-4.5755e+09j,\n",
       "         -2.0039e+02-4.5755e+09j, -2.0073e+02-4.5755e+09j,\n",
       "         -2.0040e+02-4.5755e+09j, -2.0066e+02-4.5755e+09j],\n",
       "        [-5.3706e-01-6.3641e+03j, -2.2218e-02-6.3641e+03j,\n",
       "         -1.8248e-01-6.3641e+03j, -3.9098e-01-6.3641e+03j,\n",
       "         -3.9711e-01-6.3641e+03j, -7.3232e-01-6.3641e+03j,\n",
       "         -3.9853e-01-6.3641e+03j, -6.6025e-01-6.3641e+03j],\n",
       "        [-5.3429e-01-3.1789e+03j, -1.9443e-02-3.1789e+03j,\n",
       "         -1.7971e-01-3.1789e+03j, -3.8821e-01-3.1789e+03j,\n",
       "         -3.9433e-01-3.1789e+03j, -7.2954e-01-3.1789e+03j,\n",
       "         -3.9576e-01-3.1789e+03j, -6.5748e-01-3.1789e+03j],\n",
       "        [-5.3506e-01-2.1158e+03j, -2.0217e-02-2.1158e+03j,\n",
       "         -1.8048e-01-2.1158e+03j, -3.8898e-01-2.1158e+03j,\n",
       "         -3.9511e-01-2.1158e+03j, -7.3032e-01-2.1158e+03j,\n",
       "         -3.9653e-01-2.1158e+03j, -6.5825e-01-2.1158e+03j],\n",
       "        [-5.3477e-01-1.5832e+03j, -1.9928e-02-1.5832e+03j,\n",
       "         -1.8019e-01-1.5832e+03j, -3.8869e-01-1.5832e+03j,\n",
       "         -3.9482e-01-1.5832e+03j, -7.3003e-01-1.5832e+03j,\n",
       "         -3.9624e-01-1.5832e+03j, -6.5796e-01-1.5832e+03j],\n",
       "        [-5.3489e-01-1.2628e+03j, -2.0042e-02-1.2628e+03j,\n",
       "         -1.8031e-01-1.2628e+03j, -3.8881e-01-1.2628e+03j,\n",
       "         -3.9493e-01-1.2628e+03j, -7.3014e-01-1.2628e+03j,\n",
       "         -3.9636e-01-1.2628e+03j, -6.5808e-01-1.2628e+03j],\n",
       "        [-5.3475e-01-1.0484e+03j, -1.9900e-02-1.0484e+03j,\n",
       "         -1.8017e-01-1.0484e+03j, -3.8867e-01-1.0484e+03j,\n",
       "         -3.9479e-01-1.0484e+03j, -7.3000e-01-1.0484e+03j,\n",
       "         -3.9622e-01-1.0484e+03j, -6.5794e-01-1.0484e+03j],\n",
       "        [-5.3483e-01-8.9475e+02j, -1.9983e-02-8.9475e+02j,\n",
       "         -1.8025e-01-8.9475e+02j, -3.8875e-01-8.9475e+02j,\n",
       "         -3.9487e-01-8.9475e+02j, -7.3008e-01-8.9475e+02j,\n",
       "         -3.9630e-01-8.9475e+02j, -6.5802e-01-8.9475e+02j],\n",
       "        [-5.3480e-01-7.7895e+02j, -1.9953e-02-7.7895e+02j,\n",
       "         -1.8022e-01-7.7895e+02j, -3.8872e-01-7.7895e+02j,\n",
       "         -3.9484e-01-7.7895e+02j, -7.3005e-01-7.7895e+02j,\n",
       "         -3.9627e-01-7.7895e+02j, -6.5799e-01-7.7895e+02j],\n",
       "        [-5.3479e-01-6.8840e+02j, -1.9941e-02-6.8840e+02j,\n",
       "         -1.8021e-01-6.8840e+02j, -3.8871e-01-6.8840e+02j,\n",
       "         -3.9483e-01-6.8840e+02j, -7.3004e-01-6.8840e+02j,\n",
       "         -3.9626e-01-6.8840e+02j, -6.5798e-01-6.8840e+02j],\n",
       "        [-5.3483e-01-6.1554e+02j, -1.9983e-02-6.1554e+02j,\n",
       "         -1.8025e-01-6.1554e+02j, -3.8875e-01-6.1554e+02j,\n",
       "         -3.9487e-01-6.1554e+02j, -7.3008e-01-6.1554e+02j,\n",
       "         -3.9630e-01-6.1554e+02j, -6.5802e-01-6.1554e+02j],\n",
       "        [-5.3483e-01-5.5552e+02j, -1.9986e-02-5.5552e+02j,\n",
       "         -1.8025e-01-5.5552e+02j, -3.8875e-01-5.5552e+02j,\n",
       "         -3.9487e-01-5.5552e+02j, -7.3008e-01-5.5552e+02j,\n",
       "         -3.9630e-01-5.5552e+02j, -6.5802e-01-5.5552e+02j],\n",
       "        [-5.3481e-01-5.0514e+02j, -1.9964e-02-5.0514e+02j,\n",
       "         -1.8023e-01-5.0514e+02j, -3.8873e-01-5.0514e+02j,\n",
       "         -3.9485e-01-5.0514e+02j, -7.3006e-01-5.0514e+02j,\n",
       "         -3.9628e-01-5.0514e+02j, -6.5800e-01-5.0514e+02j],\n",
       "        [-5.3480e-01-4.6217e+02j, -1.9950e-02-4.6217e+02j,\n",
       "         -1.8022e-01-4.6217e+02j, -3.8872e-01-4.6217e+02j,\n",
       "         -3.9484e-01-4.6217e+02j, -7.3005e-01-4.6217e+02j,\n",
       "         -3.9627e-01-4.6217e+02j, -6.5799e-01-4.6217e+02j],\n",
       "        [-5.3480e-01-4.2502e+02j, -1.9950e-02-4.2502e+02j,\n",
       "         -1.8022e-01-4.2502e+02j, -3.8871e-01-4.2502e+02j,\n",
       "         -3.9484e-01-4.2502e+02j, -7.3005e-01-4.2502e+02j,\n",
       "         -3.9627e-01-4.2502e+02j, -6.5799e-01-4.2502e+02j],\n",
       "        [-5.3478e-01-3.9252e+02j, -1.9934e-02-3.9252e+02j,\n",
       "         -1.8020e-01-3.9252e+02j, -3.8870e-01-3.9252e+02j,\n",
       "         -3.9482e-01-3.9252e+02j, -7.3003e-01-3.9252e+02j,\n",
       "         -3.9625e-01-3.9252e+02j, -6.5797e-01-3.9252e+02j],\n",
       "        [-5.3480e-01-3.6380e+02j, -1.9950e-02-3.6380e+02j,\n",
       "         -1.8022e-01-3.6380e+02j, -3.8872e-01-3.6380e+02j,\n",
       "         -3.9484e-01-3.6380e+02j, -7.3005e-01-3.6380e+02j,\n",
       "         -3.9627e-01-3.6380e+02j, -6.5799e-01-3.6380e+02j],\n",
       "        [-5.3481e-01-3.3818e+02j, -1.9961e-02-3.3818e+02j,\n",
       "         -1.8023e-01-3.3818e+02j, -3.8873e-01-3.3818e+02j,\n",
       "         -3.9485e-01-3.3818e+02j, -7.3006e-01-3.3818e+02j,\n",
       "         -3.9628e-01-3.3818e+02j, -6.5800e-01-3.3818e+02j],\n",
       "        [-5.3481e-01-3.1515e+02j, -1.9960e-02-3.1515e+02j,\n",
       "         -1.8023e-01-3.1515e+02j, -3.8872e-01-3.1515e+02j,\n",
       "         -3.9485e-01-3.1515e+02j, -7.3006e-01-3.1515e+02j,\n",
       "         -3.9628e-01-3.1515e+02j, -6.5800e-01-3.1515e+02j],\n",
       "        [-5.3481e-01-2.9429e+02j, -1.9964e-02-2.9429e+02j,\n",
       "         -1.8023e-01-2.9429e+02j, -3.8873e-01-2.9429e+02j,\n",
       "         -3.9485e-01-2.9429e+02j, -7.3006e-01-2.9429e+02j,\n",
       "         -3.9628e-01-2.9429e+02j, -6.5800e-01-2.9429e+02j],\n",
       "        [-5.3480e-01-2.7528e+02j, -1.9949e-02-2.7528e+02j,\n",
       "         -1.8021e-01-2.7528e+02j, -3.8871e-01-2.7528e+02j,\n",
       "         -3.9484e-01-2.7528e+02j, -7.3005e-01-2.7528e+02j,\n",
       "         -3.9627e-01-2.7528e+02j, -6.5798e-01-2.7528e+02j],\n",
       "        [-5.3481e-01-2.5784e+02j, -1.9967e-02-2.5784e+02j,\n",
       "         -1.8023e-01-2.5784e+02j, -3.8873e-01-2.5784e+02j,\n",
       "         -3.9486e-01-2.5784e+02j, -7.3007e-01-2.5784e+02j,\n",
       "         -3.9628e-01-2.5784e+02j, -6.5800e-01-2.5784e+02j],\n",
       "        [-5.3481e-01-2.4176e+02j, -1.9962e-02-2.4176e+02j,\n",
       "         -1.8023e-01-2.4176e+02j, -3.8873e-01-2.4176e+02j,\n",
       "         -3.9485e-01-2.4176e+02j, -7.3006e-01-2.4176e+02j,\n",
       "         -3.9628e-01-2.4176e+02j, -6.5800e-01-2.4176e+02j],\n",
       "        [-5.3482e-01-2.2686e+02j, -1.9969e-02-2.2686e+02j,\n",
       "         -1.8024e-01-2.2686e+02j, -3.8873e-01-2.2686e+02j,\n",
       "         -3.9486e-01-2.2686e+02j, -7.3007e-01-2.2686e+02j,\n",
       "         -3.9629e-01-2.2686e+02j, -6.5800e-01-2.2686e+02j],\n",
       "        [-5.3481e-01-2.1298e+02j, -1.9960e-02-2.1298e+02j,\n",
       "         -1.8023e-01-2.1298e+02j, -3.8873e-01-2.1298e+02j,\n",
       "         -3.9485e-01-2.1298e+02j, -7.3006e-01-2.1298e+02j,\n",
       "         -3.9628e-01-2.1298e+02j, -6.5800e-01-2.1298e+02j],\n",
       "        [-5.3481e-01-2.0000e+02j, -1.9963e-02-2.0000e+02j,\n",
       "         -1.8023e-01-2.0000e+02j, -3.8873e-01-2.0000e+02j,\n",
       "         -3.9485e-01-2.0000e+02j, -7.3006e-01-2.0000e+02j,\n",
       "         -3.9628e-01-2.0000e+02j, -6.5800e-01-2.0000e+02j],\n",
       "        [-5.3482e-01-1.8781e+02j, -1.9971e-02-1.8781e+02j,\n",
       "         -1.8024e-01-1.8781e+02j, -3.8874e-01-1.8781e+02j,\n",
       "         -3.9486e-01-1.8781e+02j, -7.3007e-01-1.8781e+02j,\n",
       "         -3.9629e-01-1.8781e+02j, -6.5801e-01-1.8781e+02j],\n",
       "        [-5.3482e-01-1.7632e+02j, -1.9969e-02-1.7632e+02j,\n",
       "         -1.8023e-01-1.7632e+02j, -3.8873e-01-1.7632e+02j,\n",
       "         -3.9486e-01-1.7632e+02j, -7.3007e-01-1.7632e+02j,\n",
       "         -3.9629e-01-1.7632e+02j, -6.5800e-01-1.7632e+02j],\n",
       "        [-5.3481e-01-1.6545e+02j, -1.9961e-02-1.6545e+02j,\n",
       "         -1.8023e-01-1.6545e+02j, -3.8873e-01-1.6545e+02j,\n",
       "         -3.9485e-01-1.6545e+02j, -7.3006e-01-1.6545e+02j,\n",
       "         -3.9628e-01-1.6545e+02j, -6.5800e-01-1.6545e+02j],\n",
       "        [-5.3481e-01-1.5514e+02j, -1.9961e-02-1.5514e+02j,\n",
       "         -1.8023e-01-1.5514e+02j, -3.8873e-01-1.5514e+02j,\n",
       "         -3.9485e-01-1.5514e+02j, -7.3006e-01-1.5514e+02j,\n",
       "         -3.9628e-01-1.5514e+02j, -6.5800e-01-1.5514e+02j],\n",
       "        [-5.3481e-01-1.4531e+02j, -1.9962e-02-1.4531e+02j,\n",
       "         -1.8023e-01-1.4531e+02j, -3.8873e-01-1.4531e+02j,\n",
       "         -3.9485e-01-1.4531e+02j, -7.3006e-01-1.4531e+02j,\n",
       "         -3.9628e-01-1.4531e+02j, -6.5800e-01-1.4531e+02j],\n",
       "        [-5.3481e-01-1.3592e+02j, -1.9964e-02-1.3592e+02j,\n",
       "         -1.8023e-01-1.3592e+02j, -3.8873e-01-1.3592e+02j,\n",
       "         -3.9485e-01-1.3592e+02j, -7.3006e-01-1.3592e+02j,\n",
       "         -3.9628e-01-1.3592e+02j, -6.5800e-01-1.3592e+02j],\n",
       "        [-5.3481e-01-1.2692e+02j, -1.9965e-02-1.2692e+02j,\n",
       "         -1.8023e-01-1.2692e+02j, -3.8873e-01-1.2692e+02j,\n",
       "         -3.9485e-01-1.2692e+02j, -7.3006e-01-1.2692e+02j,\n",
       "         -3.9628e-01-1.2692e+02j, -6.5800e-01-1.2692e+02j],\n",
       "        [-5.3481e-01-1.1828e+02j, -1.9963e-02-1.1828e+02j,\n",
       "         -1.8023e-01-1.1828e+02j, -3.8873e-01-1.1828e+02j,\n",
       "         -3.9485e-01-1.1828e+02j, -7.3006e-01-1.1828e+02j,\n",
       "         -3.9628e-01-1.1828e+02j, -6.5800e-01-1.1828e+02j],\n",
       "        [-5.3481e-01-1.0995e+02j, -1.9962e-02-1.0995e+02j,\n",
       "         -1.8023e-01-1.0995e+02j, -3.8873e-01-1.0995e+02j,\n",
       "         -3.9485e-01-1.0995e+02j, -7.3006e-01-1.0995e+02j,\n",
       "         -3.9628e-01-1.0995e+02j, -6.5800e-01-1.0995e+02j],\n",
       "        [-5.3481e-01-1.0191e+02j, -1.9962e-02-1.0191e+02j,\n",
       "         -1.8023e-01-1.0191e+02j, -3.8873e-01-1.0191e+02j,\n",
       "         -3.9485e-01-1.0191e+02j, -7.3006e-01-1.0191e+02j,\n",
       "         -3.9628e-01-1.0191e+02j, -6.5800e-01-1.0191e+02j],\n",
       "        [-5.3481e-01-9.4113e+01j, -1.9964e-02-9.4113e+01j,\n",
       "         -1.8023e-01-9.4113e+01j, -3.8873e-01-9.4113e+01j,\n",
       "         -3.9485e-01-9.4113e+01j, -7.3006e-01-9.4113e+01j,\n",
       "         -3.9628e-01-9.4113e+01j, -6.5800e-01-9.4113e+01j],\n",
       "        [-5.3481e-01-8.6548e+01j, -1.9962e-02-8.6548e+01j,\n",
       "         -1.8023e-01-8.6548e+01j, -3.8873e-01-8.6548e+01j,\n",
       "         -3.9485e-01-8.6548e+01j, -7.3006e-01-8.6548e+01j,\n",
       "         -3.9628e-01-8.6548e+01j, -6.5800e-01-8.6548e+01j],\n",
       "        [-5.3481e-01-7.9186e+01j, -1.9960e-02-7.9186e+01j,\n",
       "         -1.8023e-01-7.9186e+01j, -3.8872e-01-7.9186e+01j,\n",
       "         -3.9485e-01-7.9186e+01j, -7.3006e-01-7.9186e+01j,\n",
       "         -3.9628e-01-7.9186e+01j, -6.5800e-01-7.9186e+01j],\n",
       "        [-5.3481e-01-7.2004e+01j, -1.9960e-02-7.2004e+01j,\n",
       "         -1.8023e-01-7.2004e+01j, -3.8873e-01-7.2004e+01j,\n",
       "         -3.9485e-01-7.2004e+01j, -7.3006e-01-7.2004e+01j,\n",
       "         -3.9628e-01-7.2004e+01j, -6.5800e-01-7.2004e+01j],\n",
       "        [-5.3481e-01-6.4984e+01j, -1.9965e-02-6.4984e+01j,\n",
       "         -1.8023e-01-6.4984e+01j, -3.8873e-01-6.4984e+01j,\n",
       "         -3.9485e-01-6.4984e+01j, -7.3006e-01-6.4984e+01j,\n",
       "         -3.9628e-01-6.4984e+01j, -6.5800e-01-6.4984e+01j],\n",
       "        [-5.3481e-01-5.8105e+01j, -1.9964e-02-5.8105e+01j,\n",
       "         -1.8023e-01-5.8105e+01j, -3.8873e-01-5.8105e+01j,\n",
       "         -3.9485e-01-5.8105e+01j, -7.3006e-01-5.8105e+01j,\n",
       "         -3.9628e-01-5.8105e+01j, -6.5800e-01-5.8105e+01j],\n",
       "        [-5.3481e-01-5.1351e+01j, -1.9962e-02-5.1351e+01j,\n",
       "         -1.8023e-01-5.1351e+01j, -3.8873e-01-5.1351e+01j,\n",
       "         -3.9485e-01-5.1351e+01j, -7.3006e-01-5.1351e+01j,\n",
       "         -3.9628e-01-5.1351e+01j, -6.5800e-01-5.1351e+01j],\n",
       "        [-5.3481e-01-4.4705e+01j, -1.9963e-02-4.4705e+01j,\n",
       "         -1.8023e-01-4.4705e+01j, -3.8873e-01-4.4705e+01j,\n",
       "         -3.9485e-01-4.4705e+01j, -7.3006e-01-4.4705e+01j,\n",
       "         -3.9628e-01-4.4705e+01j, -6.5800e-01-4.4705e+01j],\n",
       "        [-5.3481e-01-3.8152e+01j, -1.9963e-02-3.8152e+01j,\n",
       "         -1.8023e-01-3.8152e+01j, -3.8873e-01-3.8152e+01j,\n",
       "         -3.9485e-01-3.8152e+01j, -7.3006e-01-3.8152e+01j,\n",
       "         -3.9628e-01-3.8152e+01j, -6.5800e-01-3.8152e+01j],\n",
       "        [-5.3481e-01-3.1677e+01j, -1.9961e-02-3.1677e+01j,\n",
       "         -1.8023e-01-3.1677e+01j, -3.8873e-01-3.1677e+01j,\n",
       "         -3.9485e-01-3.1677e+01j, -7.3006e-01-3.1677e+01j,\n",
       "         -3.9628e-01-3.1677e+01j, -6.5800e-01-3.1677e+01j],\n",
       "        [-5.3481e-01-2.5266e+01j, -1.9961e-02-2.5266e+01j,\n",
       "         -1.8023e-01-2.5266e+01j, -3.8873e-01-2.5266e+01j,\n",
       "         -3.9485e-01-2.5266e+01j, -7.3006e-01-2.5266e+01j,\n",
       "         -3.9628e-01-2.5266e+01j, -6.5800e-01-2.5266e+01j],\n",
       "        [-5.3481e-01-1.8906e+01j, -1.9964e-02-1.8906e+01j,\n",
       "         -1.8023e-01-1.8906e+01j, -3.8873e-01-1.8906e+01j,\n",
       "         -3.9485e-01-1.8906e+01j, -7.3006e-01-1.8906e+01j,\n",
       "         -3.9628e-01-1.8906e+01j, -6.5800e-01-1.8906e+01j],\n",
       "        [-5.3481e-01-1.2583e+01j, -1.9963e-02-1.2583e+01j,\n",
       "         -1.8023e-01-1.2583e+01j, -3.8873e-01-1.2583e+01j,\n",
       "         -3.9485e-01-1.2583e+01j, -7.3006e-01-1.2583e+01j,\n",
       "         -3.9628e-01-1.2583e+01j, -6.5800e-01-1.2583e+01j],\n",
       "        [-5.3481e-01-6.2852e+00j, -1.9962e-02-6.2852e+00j,\n",
       "         -1.8023e-01-6.2852e+00j, -3.8873e-01-6.2852e+00j,\n",
       "         -3.9485e-01-6.2852e+00j, -7.3006e-01-6.2852e+00j,\n",
       "         -3.9628e-01-6.2852e+00j, -6.5800e-01-6.2852e+00j]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b38a9376-d908-4473-b504-6d4ba5e6d887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9976],\n",
       "        [0.3359],\n",
       "        [0.3027],\n",
       "        [0.2121],\n",
       "        [0.8180],\n",
       "        [0.3799],\n",
       "        [0.3188],\n",
       "        [0.9348],\n",
       "        [0.4519],\n",
       "        [0.7388]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa241b8a-7455-422e-9ce7-2d8c192eb76e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mg\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12d74d7c-0659-444e-8f5b-f6f24309ec39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f56321fc-5778-4b35-931a-0ff229106b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_DPLR(\n",
    "    Lambda : torch.Tensor,\n",
    "    P : torch.Tensor,\n",
    "    Q : torch.Tensor,\n",
    "    B : torch.Tensor,\n",
    "    C : torch.Tensor,\n",
    "    delta : torch.Tensor,\n",
    "    L : int\n",
    ")->(torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    computes the discretized version of the state space model,\n",
    "    assuming the DPLR form\n",
    "\n",
    "    Parameters:\n",
    "        Lambda : Nx1, represents the diagonal values of the A matrix\n",
    "        P : Nx1, represents part of the low rank aspect of the A matrix\n",
    "        Q : Nx1, represents the other part of the low rank aspect of the A matrix\n",
    "        B : N, projection from input to latent\n",
    "        C : N, projection from latent to input\n",
    "        delta : step size\n",
    "        L : length of window\n",
    "    \"\"\"\n",
    "    Bt = B.unsqueeze(1)\n",
    "    Ct = C.unsqueeze(0)\n",
    "\n",
    "    A = (torch.diag(Lambda) - torch.outer(P, torch.conj(Q)))\n",
    "    A0 = 2.0/delta * torch.eye(A.shape[0]) + A\n",
    "\n",
    "    Qdagger = torch.conj(torch.transpose(Q))\n",
    "    \n",
    "    D = torch.diag(1.0/(2.0/delta - Lambda))\n",
    "    A1 = (D -  (1.0/(1.0 + Qdagger @ D @ P)) * D@P@Qdagger@D)\n",
    "    Ab = A1@A0\n",
    "    Bb = 2 * A1\n",
    "    Cb = Ct @ torch.conj(torch.linalg.inv(torch.eye(A.shape[0]) - torch.matrix_power(Ab, L)))\n",
    "    return Ab, Bb, Cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36b778f2-3e39-4bd6-821a-032e7f0c3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_NPLR_HiPPO(N : int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    creating hippo matrix and associated low rank additive component, P\n",
    "    and the B matrix associated, as hippo forces it\n",
    "\n",
    "    parameters:\n",
    "        N : int, degree of legendre polynomial coefficient\n",
    "    \"\"\"\n",
    "    nhippo = make_HiPPO(N)\n",
    "\n",
    "    P = torch.sqrt(torch.arange(N)+0.5).to(torch.complex64)\n",
    "    B = torch.sqrt(2*torch.arange(N)+1.0).to(torch.complex64)\n",
    "\n",
    "    return nhippo.to(torch.complex64), P, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5c323dd-ae2a-4a55-b903-b0d347e15699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DPLR_HiPPO(N : int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    convert matrices to DPLR representation\n",
    "    parameters:\n",
    "        N : int, degree of legendre polynomials\n",
    "    \"\"\"\n",
    "    A, P, B = make_NPLR_HiPPO(N)\n",
    "\n",
    "    S = A + torch.outer(P, P)\n",
    "\n",
    "    S_diag = torch.diagonal(S)\n",
    "    Lambda_real = torch.mean(S_diag) * torch.ones_like(S_diag)\n",
    "\n",
    "    Lambda_imag, V = torch.linalg.eigh(S * -1j)\n",
    "    P = V.T.conj() @ P\n",
    "    B = V.T.conj() @ B\n",
    "    return Lambda_real + 1j * Lambda_imag, P, B, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7e5519a-b02a-43d6-9700-4c5377371354",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=8\n",
    "A2, P, B = make_NPLR_HiPPO(N)\n",
    "Lambda, Pc, Bc, V = make_DPLR_HiPPO(N)\n",
    "Vc = V.conj().T\n",
    "P = P\n",
    "Pc = Pc\n",
    "Lambda = torch.diag(Lambda)\n",
    "A3 = V @ Lambda @ Vc - torch.outer(P,P.conj())  # Test NPLR\n",
    "A4 = V @ (Lambda - torch.outer(Pc,Pc.conj())) @ Vc  # Test DPLR\n",
    "\n",
    "assert torch.allclose(A2, A3, atol=1e-4, rtol=1e-4)\n",
    "assert torch.allclose(A2, A4, atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f9db858-b305-4b72-9d3a-782d6e25f26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a2785f4-165e-4c48-8ee6-728f135b3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1140, -1.3116,  0.7624,  0.3642,  0.8779,  0.8805,  1.0080, -1.9108,\n",
       "        -0.1149, -1.4994])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(10,10)\n",
    "torch.nn.init.normal_(torch.empty(10), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc29f66c-4ca6-43a4-9e1c-1d37798c5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Layer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient layer for S4Ms. (Structured State Space Sequence Models).\n",
    "    Implements initialization of A as a NPLR matrix, enabling fast \n",
    "    matrix vector multiplication. \n",
    "\n",
    "    Several parameters, such as the projection matrix, are learned.\n",
    "\n",
    "    In this case, the C matrix is actually learned as C(1-A^L). \n",
    "    This is fairly easy to undo, and is done in the calc of Cbar.\n",
    "\n",
    "    Parameters:\n",
    "        N_input : dimension of input,\n",
    "        latent_dim : int, dimensions of latent space,\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        N_input : int,\n",
    "        latent_dim : int,\n",
    "        dt_min  : float = 0.001,\n",
    "        dt_max  : float = 0.1,\n",
    "        step_grad : bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert N_input==1\n",
    "\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.Lambda, self.P, self.B, _ = make_DPLR_HiPPO(self.latent_dim)\n",
    "        \n",
    "        self.Lambda = torch.autograd.Variable(self.Lambda, requires_grad = True)\n",
    "        self.P = torch.autograd.Variable(self.P, requires_grad = True)\n",
    "        self.B = torch.autograd.Variable(self.B, requires_grad = True)\n",
    "        \n",
    "        self.dt = torch.exp(self.log_step_initializer(dt_min, dt_max, step_grad))\n",
    "        \n",
    "        Ctilde = torch.nn.init.normal_(torch.empty(self.latent_dim, 2), mean=0, std=0.5**0.5)\n",
    "        self.Ctilde = torch.autograd.Variable(Ctilde[:,0] + Ctilde[:,1]*1j, requires_grad=True)\n",
    "\n",
    "        self.D = torch.autograd.Variable(torch.tensor(1), requires_grad = True)\n",
    "\n",
    "    @staticmethod\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_DPLR_HiPPO(N : int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        convert matrices to DPLR representation\n",
    "        parameters:\n",
    "            N : int, degree of legendre polynomials\n",
    "        \"\"\"\n",
    "        A, P, B = make_NPLR_HiPPO(N)\n",
    "    \n",
    "        S = A + torch.outer(P, P)\n",
    "    \n",
    "        S_diag = torch.diagonal(S)\n",
    "        Lambda_real = torch.mean(S_diag) * torch.ones_like(S_diag)\n",
    "    \n",
    "        Lambda_imag, V = torch.linalg.eigh(S * -1j)\n",
    "        P = V.T.conj() @ P\n",
    "        B = V.T.conj() @ B\n",
    "        return Lambda_real + 1j * Lambda_imag, P, B, V\n",
    "\n",
    "    \n",
    "    def log_step_initializer(self, dt_min = 0.001, dt_max = 0.1, requires_grad = True):\n",
    "        \"\"\"\n",
    "        initial guess for dt, from random number generator. to be learned.\n",
    "    \n",
    "        parameters:\n",
    "            dt_min\n",
    "            dt_max\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.rand(1) * (torch.log(torch.tensor(dt_max)) - torch.log(torch.tensor(dt_min))) + torch.log(torch.tensor(dt_min)), requires_grad = requires_grad)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def cauchy_dot(v, omega, lambd):\n",
    "        print(v.shape)\n",
    "        print(omega.shape)\n",
    "        print(lambd.shape)\n",
    "        return (v / (omega - lambd)).sum()\n",
    "\n",
    "    \n",
    "    def kernel_DPLR(\n",
    "        self,\n",
    "        Lambda : torch.Tensor, \n",
    "        P : torch.Tensor, \n",
    "        Q : torch.Tensor, \n",
    "        B: torch.Tensor, \n",
    "        C : torch.Tensor, \n",
    "        delta : torch.Tensor, \n",
    "        L : int\n",
    "    )-> torch.Tensor:\n",
    "        \"\"\"\n",
    "        computes convolution kernel from generating function using DPLR representation and\n",
    "        the cauchy kernel with CTilde assumed, not C.\n",
    "    \n",
    "        Parameters:\n",
    "            Lambda : diagonal part of A\n",
    "            P : Nx1 matrix, rank 1 representation to A\n",
    "            Q : Nx1 matrix, rank 1 representation to A\n",
    "            C : 1xN matrix, projection from latent to input\n",
    "            B : Nx1 matrix, projection from input to latent\n",
    "        \"\"\"\n",
    "        Omega_L = torch.exp(-2j*torch.pi * (torch.arange(L))/L)\n",
    "    \n",
    "        aterm = (torch.conj(C), torch.conj(Q))\n",
    "        bterm = (B, P)\n",
    "    \n",
    "        g = (2.0/delta) * ((1.0-Omega_L)/(1.0+Omega_L))\n",
    "        c = 2.0 / (1.0+Omega_L)\n",
    "\n",
    "        Lambda = Lambda.unsqueeze(0)\n",
    "    \n",
    "        k00 = self.cauchy_dot(aterm[0] * bterm[0], g, Lambda)\n",
    "        k01 = self.cauchy_dot(aterm[0] * bterm[1], g, Lambda)\n",
    "        k10 = self.cauchy_dot(aterm[1] * bterm[0], g, Lambda)\n",
    "        k11 = self.cauchy_dot(aterm[1] * bterm[1], g, Lambda)\n",
    "    \n",
    "        atRoots = c * (k00 - k01 * (1.0 / (1.0 + k11)) * k10)\n",
    "        out = np.fft.irfft(atRoots, L)\n",
    "        return out\n",
    "\n",
    "    def discrete_DPLR(\n",
    "        self,\n",
    "        Lambda : torch.Tensor,\n",
    "        P : torch.Tensor,\n",
    "        Q : torch.Tensor,\n",
    "        B : torch.Tensor,\n",
    "        C : torch.Tensor,\n",
    "        delta : torch.Tensor,\n",
    "        L : int\n",
    "    )->(torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        computes the discretized version of the state space model,\n",
    "        assuming the DPLR form with Ctilde, not C\n",
    "    \n",
    "        Parameters:\n",
    "            Lambda : Nx1, represents the diagonal values of the A matrix\n",
    "            P : Nx1, represents part of the low rank aspect of the A matrix\n",
    "            Q : Nx1, represents the other part of the low rank aspect of the A matrix\n",
    "            B : N, projection from input to latent\n",
    "            C : N, projection from latent to input\n",
    "            delta : step size\n",
    "            L : length of window\n",
    "        \"\"\"\n",
    "        Bt = B.unsqueeze(1)\n",
    "        Ct = C.unsqueeze(0)\n",
    "    \n",
    "        A = (torch.diag(Lambda) - torch.outer(P, torch.conj(Q)))\n",
    "        A0 = 2.0/delta * torch.eye(A.shape[0]) + A\n",
    "    \n",
    "        Qdagger = torch.conj(Q.T)\n",
    "        \n",
    "        D = torch.diag(1.0/(2.0/delta - Lambda))\n",
    "        A1 = (D -  (1.0/(1.0 + Qdagger @ D @ P)) * D@P@Qdagger@D)\n",
    "        Ab = A1@A0\n",
    "        Bb = 2 * A1\n",
    "        Cb = Ct @ torch.conj(torch.linalg.inv(torch.eye(A.shape[0]) - torch.matrix_power(Ab, L)))\n",
    "        return Ab, Bb, Cb\n",
    "\n",
    "    def forward(self, u : torch.Tensor, x0 : torch.Tensor, mode : bool | str):\n",
    "        L = u.shape[0]\n",
    "        if mode not in [\"recurrent\", True, False]:\n",
    "            raise('mode not valid')\n",
    "            \n",
    "        if mode == \"recurrent\":\n",
    "            Ab, Bb, Cb = self.discrete_DPLR(self.Lambda, self.P, self.P, self.B, self.C, self.dt, L)\n",
    "            return self.scan_SSM(Ab, Bb, Cb, self.D, u, x0)[1]\n",
    "        else:\n",
    "            if mode:\n",
    "                warnings.warn(\"convolving in non-fft mode. this is not recommended, as it is slow for large L\")\n",
    "            K = self.kernel_DPLR(self.Lambda, self.P, self.P, self.B, self.C, self.dt, L)\n",
    "            return self.causal_conv(u, K, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a28800e2-fb15-4199-afde-bc0fc7307373",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = S4Layer(N_input = 1, latent_dim = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e6bc002-b28e-4813-a7e3-4e0cccacf170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([10000])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10000) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/s4ms/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/s4ms/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[110], line 166\u001b[0m, in \u001b[0;36mS4Layer.forward\u001b[0;34m(self, u, x0, mode)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode:\n\u001b[1;32m    165\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvolving in non-fft mode. this is not recommended, as it is slow for large L\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_DPLR\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausal_conv(u, K, mode)\n",
      "Cell \u001b[0;32mIn[110], line 108\u001b[0m, in \u001b[0;36mS4Layer.kernel_DPLR\u001b[0;34m(self, Lambda, P, Q, B, C, delta, L)\u001b[0m\n\u001b[1;32m    104\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m+\u001b[39mOmega_L)\n\u001b[1;32m    106\u001b[0m Lambda \u001b[38;5;241m=\u001b[39m Lambda\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m k00 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcauchy_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43materm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbterm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m k01 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcauchy_dot(aterm[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m bterm[\u001b[38;5;241m1\u001b[39m], g, Lambda)\n\u001b[1;32m    110\u001b[0m k10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcauchy_dot(aterm[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m bterm[\u001b[38;5;241m0\u001b[39m], g, Lambda)\n",
      "Cell \u001b[0;32mIn[110], line 74\u001b[0m, in \u001b[0;36mS4Layer.cauchy_dot\u001b[0;34m(v, omega, lambd)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(omega\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(lambd\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (v \u001b[38;5;241m/\u001b[39m (\u001b[43momega\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambd\u001b[49m))\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10000) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "test_layer(u, torch.tensor(0), mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "57fa4473-c775-4b2b-9f41-1d455c4d4415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cauchy(v: torch.Tensor, omega: torch.Tensor, lambd: torch.Tensor) -> torch.Tensor>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9923fa4-32c1-493c-8e07-12631a3a9ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
